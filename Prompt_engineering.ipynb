{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirTee12/LLM-Kaggle-Google-GenAI/blob/main/Prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ85fhKIppqo"
      },
      "source": [
        "## Install the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7_t8fZxbZMh",
        "outputId": "93d95179-aa6a-4c11-b9bd-b7483d82050d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab # uninstall jupyterlab in the case it conflict google colab base image\n",
        "!pip install -U -q \"google-genai==1.7.0\" # install google genai library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkGlMbU9ueF5"
      },
      "source": [
        "import the SDK and some helpers for rendering the output. The `types` in `import types` may include custom data structures, classes or type hints psecifically to work with google AI. These custom types might represent things like model parameters, input/output formats for AI models or config settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vn5P5o2Zp4Jd"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import enum\n",
        "import io\n",
        "from IPython.display import HTML, Markdown, display, clear_output\n",
        "import typing_extensions as typing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCueWvEs3iyF"
      },
      "source": [
        "## API Call Retry Implementation\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota\n",
        "\n",
        "\n",
        "1.  **Imports `retry`:** It imports the `retry` module from `google.api_core`, which provides tools for automatic retries of API calls.\n",
        "\n",
        "2.  **Defines `is_retriable`:**\n",
        "    * It creates a lambda function called `is_retriable`.\n",
        "    * This function checks if an exception (`e`) is an instance of `genai.errors.APIErrpr` and if its error code (`e.code`) is either 429 (Too Many Requests) or 503 (Service Unavailable).\n",
        "    * It returns `True` if both conditions are met (meaning the API call is considered retriable), and `False` otherwise.\n",
        "\n",
        "3.  **Applies Retry Logic:**\n",
        "    * It modifies the `genai.models.Models.generate_content` function by wrapping it with the `retry.retry` function.\n",
        "    * The `retry.retry` function is configured with the `is_retriable` function as the `predicate`. This tells `retry.retry` to only retry the API call if the `is_retriable` function returns `True` for the encountered exception.\n",
        "    * Essentially, this replaces the original `generate_content` function with a new version that automatically retries on 429 and 503 errors, making the code more robust to temporary API issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tyJcQ1EW3tec"
      },
      "outputs": [],
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429, 503))\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate = is_retriable)(genai.models.Models.generate_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jzS_Om16DxV"
      },
      "outputs": [],
      "source": [
        "#from kaggle_secrets import UserSecretsClient\n",
        "#user_secrets = UserSecretsClient()\n",
        "#secret_value_0 = user_secrets.get_secret(\"secret-label-prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmMX2ZA28UP_"
      },
      "source": [
        "## Get the API keys and create a client\n",
        "\n",
        "Retrieves the API key securely and create a client object that will be used to communicate with the generative AI service, using the retrieved API key for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VpfMD3Jo7_IX"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # retrive the APi key\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)   # create a client to interact with the genai application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaCFpBre_3sF"
      },
      "source": [
        "## Running the First Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-zXMiooZ_pMB",
        "outputId": "2ae8c78a-1254-4eb3-bd74-0cec5d1b3082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really smart puppy named Sparky.\n",
            "\n",
            "Sparky doesn't know anything when he's born. You have to teach him everything!\n",
            "\n",
            "**AI is like teaching a computer to be like Sparky, but instead of teaching him tricks like \"sit\" or \"fetch,\" you teach him to do things like:**\n",
            "\n",
            "*   **Recognize things:** You show Sparky lots of pictures of cats and dogs and tell him which is which.  After a while, he learns to tell them apart!  AI can do the same thing, but with anything you show it, like faces, cars, or even different types of fruit!\n",
            "\n",
            "*   **Learn from mistakes:**  When Sparky chews your shoe, you tell him \"no!\".  He learns not to do that again.  AI can also learn from its mistakes.  If it guesses the wrong answer, you can tell it, and it will try to do better next time.\n",
            "\n",
            "*   **Solve problems:**  Maybe you teach Sparky to find a hidden toy.  AI can also solve problems, like figuring out the best way to get to school or writing a story!\n",
            "\n",
            "**So, what makes AI different from Sparky?**\n",
            "\n",
            "*   **Super Speed:** AI can learn much, much faster than a puppy.\n",
            "*   **Huge Memory:** AI can remember way more information than Sparky.\n",
            "*   **No Treats Needed:**  You don't have to give AI treats to keep it learning!\n",
            "\n",
            "**Where do we see AI?**\n",
            "\n",
            "*   **Your phone:**  When you ask Siri or Google Assistant a question, that's AI!\n",
            "*   **Video games:**  The enemies in your games use AI to make decisions.\n",
            "*   **Movies:** Sometimes special effects artists use AI to make things look really real.\n",
            "\n",
            "**AI is like a super-smart, fast-learning helper that can do lots of amazing things! It's still learning and growing, but it's already making our lives easier and more fun!**\n",
            "\n",
            "Does that make sense?\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine you have a really smart puppy named Sparky.\n\nSparky doesn't know anything when he's born. You have to teach him everything!\n\n**AI is like teaching a computer to be like Sparky, but instead of teaching him tricks like \"sit\" or \"fetch,\" you teach him to do things like:**\n\n*   **Recognize things:** You show Sparky lots of pictures of cats and dogs and tell him which is which.  After a while, he learns to tell them apart!  AI can do the same thing, but with anything you show it, like faces, cars, or even different types of fruit!\n\n*   **Learn from mistakes:**  When Sparky chews your shoe, you tell him \"no!\".  He learns not to do that again.  AI can also learn from its mistakes.  If it guesses the wrong answer, you can tell it, and it will try to do better next time.\n\n*   **Solve problems:**  Maybe you teach Sparky to find a hidden toy.  AI can also solve problems, like figuring out the best way to get to school or writing a story!\n\n**So, what makes AI different from Sparky?**\n\n*   **Super Speed:** AI can learn much, much faster than a puppy.\n*   **Huge Memory:** AI can remember way more information than Sparky.\n*   **No Treats Needed:**  You don't have to give AI treats to keep it learning!\n\n**Where do we see AI?**\n\n*   **Your phone:**  When you ask Siri or Google Assistant a question, that's AI!\n*   **Video games:**  The enemies in your games use AI to make decisions.\n*   **Movies:** Sometimes special effects artists use AI to make things look really real.\n\n**AI is like a super-smart, fast-learning helper that can do lots of amazing things! It's still learning and growing, but it's already making our lives easier and more fun!**\n\nDoes that make sense?\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# generate a response\n",
        "response = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = \"Explain AI to me like I'm a kid\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQZTsixG3hl"
      },
      "source": [
        "## Let's start a chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvlI-MyAZNj",
        "outputId": "e064739a-a97d-4696-c2b2-98402dead145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Ahmad! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat = client.chats.create(model = \"gemini-2.0-flash\", history = [])\n",
        "response = chat.send_message('Hello, my name is Ahmad')\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZR6L3fHTEP",
        "outputId": "343e3464-b1ab-47c7-f965-de80abeacac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's an interesting fact about dinosaurs:\n",
            "\n",
            "**Some dinosaurs had feathers, even the massive ones!**\n",
            "\n",
            "It's commonly thought that only smaller, bird-like dinosaurs had feathers, but evidence is growing that many different types of dinosaurs, including some very large ones like certain types of ornithopods, may have had feathers or feathery proto-structures. These feathers likely weren't for flight (in the larger species), but rather for insulation, display, or even for tactile purposes.\n",
            "\n",
            "So, imagine a huge, plant-eating dinosaur, bigger than an elephant, covered in fluffy feathers! It's a pretty mind-blowing thought.\n",
            "\n",
            "I can tell you more if you'd like! Just let me know what aspects of dinosaurs interest you.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"can you tell me something interesting about dinosaurs\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1O0TSqHef2",
        "outputId": "979a883f-93d9-4346-8bae-82e2a217296e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I do! Your name is Ahmad.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"do you still remember my name\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUBrfruvh7mS"
      },
      "source": [
        "# Chose a model from the Gemini model family\n",
        "\n",
        "Retrieves a list of available AI models, searches for a specific model named \"gemini-2.0-flash,\" and if found, displays its details in a formatted JSON-like structure. The search stops once the target model is located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkU0vWoSglap",
        "outputId": "e21863e1-1c07-47a4-ea64-beb5b6eb4068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='models/chat-bison-001' display_name='PaLM 2 Chat (Legacy)' description='A legacy text-only model optimized for chat conversations' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=4096 output_token_limit=1024 supported_actions=['generateMessage', 'countMessageTokens']\n",
            "name='models/text-bison-001' display_name='PaLM 2 (Legacy)' description='A legacy model that understands text and generates text as an output' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8196 output_token_limit=1024 supported_actions=['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens']\n",
            "name='models/gemini-1.0-pro-vision-latest' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-pro-vision' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-latest' display_name='Gemini 1.5 Pro Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-001' display_name='Gemini 1.5 Pro 001' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro-002' display_name='Gemini 1.5 Pro 002' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro' display_name='Gemini 1.5 Pro' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-latest' display_name='Gemini 1.5 Flash Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-001' display_name='Gemini 1.5 Flash 001' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-001-tuning' display_name='Gemini 1.5 Flash 001 Tuning' description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=16384 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createTunedModel']\n",
            "name='models/gemini-1.5-flash' display_name='Gemini 1.5 Flash' description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-002' display_name='Gemini 1.5 Flash 002' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-8b' display_name='Gemini 1.5 Flash-8B' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-001' display_name='Gemini 1.5 Flash-8B 001' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-latest' display_name='Gemini 1.5 Flash-8B Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0827' display_name='Gemini 1.5 Flash 8B Experimental 0827' description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0924' display_name='Gemini 1.5 Flash 8B Experimental 0924' description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.5-pro-exp-03-25' display_name='Gemini 2.5 Pro Experimental 03-25' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.5-pro-preview-03-25' display_name='Gemini 2.5 Pro Preview 03-25' description='Gemini 2.5 Pro Preview 03-25' version='2.5-preview-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.5-flash-preview-04-17' display_name='Gemini 2.5 Flash Preview 04-17' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-04-17' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-exp-image-generation' display_name='Gemini 2.0 Flash (Image Generation) Experimental' description='Gemini 2.0 Flash (Image Generation) Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.0 Flash Thinking Experimental' description='Gemini 2.0 Flash Thinking Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/learnlm-1.5-pro-experimental' display_name='LearnLM 1.5 Pro Experimental' description='Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32767 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/learnlm-2.0-flash-experimental' display_name='LearnLM 2.0 Flash Experimental' description='LearnLM 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=32768 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer']\n",
            "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0 002 model' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predict']\n",
            "name='models/gemini-2.0-flash-live-001' display_name='Gemini 2.0 Flash 001' description='Gemini 2.0 Flash 001' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['bidiGenerateContent', 'countTokens']\n"
          ]
        }
      ],
      "source": [
        "for model in client.models.list():\n",
        "  print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmeO6A6YIe0E",
        "outputId": "fde0d205-c1cb-4086-d961-c1e0b85b98df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens', 'createCachedContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi_3neIHkzaj"
      },
      "source": [
        "## Parameters Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGm-Pg5OmE3Z"
      },
      "source": [
        "### Output Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpL4Sb2oiu7r",
        "outputId": "641a1307-f12c-4801-b968-26b4220530e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Cornerstone of Progress: Why Education Remains Vital in Modern Society\n",
            "\n",
            "In the labyrinthine complexity of modern society, amidst rapid technological advancements and ever-shifting socio-economic landscapes, one constant remains a beacon of progress and empowerment: education. While access to information has become democratized through the internet, the crucial role of education in cultivating critical thinking, fostering innovation, and ensuring equitable opportunities has only become more pronounced. It is not merely about memorizing facts; rather, education serves as the cornerstone upon which individual fulfillment and societal advancement are built.\n",
            "\n",
            "Firstly, education empowers individuals with the critical thinking skills necessary to navigate the deluge of information that characterizes the modern world. In an era rife with misinformation and biased narratives, the ability to analyze, evaluate, and discern truth from falsehood is paramount. Education provides the framework for logical reasoning, problem-solving, and independent thought, allowing individuals to become informed and engaged citizens capable of making responsible decisions. This capacity is not just beneficial for personal well-being;\n"
          ]
        }
      ],
      "source": [
        "# set the max number of output token to 200\n",
        "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
        "\n",
        "# create a response\n",
        "response_short = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = short_config,\n",
        "    contents='write a short essay on the importance of education in modern day society.'\n",
        ")\n",
        "\n",
        "print(response_short.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErIkZR5snjVC"
      },
      "source": [
        "### Temperature\n",
        "\n",
        "How much randomness is included when selecting the next word (token) in language models depends on the \"temperature\" parameter.  A higher temperature causes the model to take into account a greater number of potential words, producing more inventive and diverse results.  On the other hand, a lower temperature forces the model to adhere to the most likely terms, producing language that is more concentrated and predictable.  The model turns completely deterministic when the temperature is set to 0, always choosing the word that is the most likely.  Temperature does not, however, ensure actual randomness; rather, it is a means of directing the model toward more or less random results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVK4xyumIuX",
        "outputId": "7b4aef8b-f140-4f76-c0fd-46bcc0198ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Purple\n",
            " _________________________\n",
            "Turquoise\n",
            " _________________________\n",
            "Magenta\n",
            " _________________________\n",
            "Purple.\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ],
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zRPP_mrs-1z"
      },
      "source": [
        "The previous output shows the randomness but we would experiment with temperature value of 0. we can see from the output that it is more direct and precise and there is no randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPCPP9Mox8R",
        "outputId": "0d5e7a06-498a-4774-dd56-16743f73a391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ],
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-BK9xPuN9X"
      },
      "source": [
        "### Top P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9LE2k_7t098",
        "outputId": "e23d9d12-52c7-4dc0-a540-946a418d7504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine we're building a big network of roads for toy cars. We want our toy cars to be able to drive to any house in the neighborhood quickly and easily.\n",
            "\n",
            "OSPF is like a helpful map and a set of rules we give each intersection so the toy cars know the best way to go!\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1. **Each Intersection Knows Its Neighbors:**  Imagine each intersection has a little name tag and knows the names of the intersections right next to it. OSPF makes each router (our intersection) know who its direct neighbors (other routers) are.  They say \"Hi, I'm Intersection A!\" and the other intersections say \"Hi, I'm Intersection B!\"\n",
            "\n",
            "2. **Sharing Information:** Then, each intersection tells all the *other* intersections (not just the ones next door) about its neighbors and how long it takes (or how \"bumpy\" the road is) to get to those neighbors.  So, Intersection A might say, \"Okay everyone, I'm connected to Intersection B and it's a smooth, short drive. I'm also connected to Intersection C, but that road is a little bumpy and takes longer.\"\n",
            "\n",
            "3. **Making a Map:**  Each intersection takes all this information and builds its own map. This map shows the *whole* network of roads and how long it takes to get to any other intersection.  It's like having a Google Maps for toy cars!\n",
            "\n",
            "4. **Finding the Best Route:** Now, when a toy car wants to get from Intersection A to Intersection Z, the intersection looks at its map and finds the FASTEST path, avoiding the bumpy roads if possible.  OSPF figures out the best route to send the toy car.\n",
            "\n",
            "5. **Adapting to Changes:** What happens if a road gets blocked or a new, super-smooth road is built? Well, the intersection that notices the change immediately tells *everyone*!  Then all the intersections update their maps and find new best routes.  So even if things change, the toy cars can still find their way!\n",
            "\n",
            "**In simple terms:**\n",
            "\n",
            "*   **OSPF helps routers (intersections) talk to each other and share information about the network (roads).**\n",
            "*   **This information is used to build a map and find the fastest way to send data (toy cars) from one place to another.**\n",
            "*   **If something changes on the network (like a road closure), OSPF helps everyone update their maps and find new routes.**\n",
            "\n",
            "So, OSPF is all about being smart, sharing information, and making sure our toy cars always get to their destination quickly and reliably!  Got it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 1.0, # default config\n",
        "    top_p = 0.95       # default config\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a senior network engineer. Explain ospf to a 5 year old\"\n",
        "response_topp = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents=story_prompt\n",
        ")\n",
        "\n",
        "print(response_topp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8rqMwXfHN3_"
      },
      "source": [
        "## Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZLlvtk0HRao"
      },
      "source": [
        "### Zero-shot Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BjVXOH_m7Y",
        "outputId": "acef7112-2546-4fe1-e939-2cd5e65cfcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set the prompt parameters\n",
        "model_config = types.GenerateContentConfig(\n",
        "  temperature = 0.1,\n",
        "  top_p = 1,\n",
        "  max_output_tokens = 5\n",
        ")\n",
        "\n",
        "# set the prompt\n",
        "zero_short_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response_zero_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "\n",
        "print(response_zero_shot.text) # print the output as text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qz1KnEKK5c"
      },
      "source": [
        "#### Enum Mode\n",
        "The Sentiment enum acts as a schema or a constraint. It tells the language model that the expected output should be one of these three predefined sentiment categories.\n",
        "\n",
        "The `text/x.enum` MIME type indicates that the response should be one of the values defined in the response_schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iJTirTJpqN",
        "outputId": "9312ff3f-51b9-4fa7-db75-e366884c2837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create a new enumeration class tha inherit from enum.Enum\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'positive'\n",
        "  NEUTRAL = 'neutral'\n",
        "  NEGATIVE = 'negative'\n",
        "\n",
        "response_zero_short_enum = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "\n",
        "        # tell the model the ezpected output format and that the rsponse should\n",
        "        # be one of thos defined in the schema\n",
        "        response_mime_type = 'text/x.enum',\n",
        "\n",
        "        # set the Sentiment as schema\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "print(response_zero_shot.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mVeGOaOc4l1"
      },
      "source": [
        "### Few Short Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UOTJ-HQojw",
        "outputId": "21483ca9-cd5c-40c0-e09f-189078ed0650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = 'Give me a large with cheese and pineapple'\n",
        "\n",
        "response_one_few_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response_one_few_shot.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0OeKzzZ4-S-"
      },
      "source": [
        "#### JSON Mode\n",
        "\n",
        "`class PizzaOrder(typing.TypedDict):`: This defines a new class called PizzaOrder that inherits from typing.TypedDict. TypedDict is used to create dictionary types where the keys and their corresponding value types are known\n",
        "\n",
        "Purpose of the `TypedDict` is to act as a schema. It defines the structure and data types of the JSON output we expect from the language model. This allows us to work with the model's response in a structured and predictable way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ-0mRg3d4L3",
        "outputId": "68fbfd78-1f0b-4a72-b520-2d76a2055bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "class PizzaOrder(typing.TypedDict):\n",
        "  size:str\n",
        "  ingredients: list[str]\n",
        "  type:str\n",
        "\n",
        "response_json = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        response_mime_type = 'application/json',\n",
        "        response_schema = PizzaOrder\n",
        "    ),\n",
        "\n",
        "    contents = \"Can I have a large dessert pizza with apple and chocolate\"\n",
        ")\n",
        "\n",
        "print(response_json.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCwQgelU1eXp"
      },
      "source": [
        "## Chain of Thought\n",
        "\n",
        "Chain of Thought prompting improves the language model's ability to perform complex reasoning tasks. By explicitly requesting the model to show its reasoning process, it's more likely to arrive at the correct answer, as it avoids impulsive or incorrect conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "2QOblph7-QL1",
        "outputId": "1a50be52-4d98-45f5-97b1-f85bfcf955a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3. **Determine your partner's current age:** Since your partner is 8 years older than you, and you are now 20, your partner is currently 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# set up the prompt\n",
        "cot_prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? let's think step by step.\"\"\"\n",
        "\n",
        "response_cot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = cot_prompt)\n",
        "\n",
        "Markdown(response_cot.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxssztRDiJx"
      },
      "source": [
        "## Reason and Act: ReAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lwSgOshf2BqE"
      },
      "outputs": [],
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd-5HwoKEhWu",
        "outputId": "87543f48-a723-4d84-d7a7-d0a5a69fac4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the Transformers NLP paper, then find the authors listed, and then find the youngest author.\n",
            "\n",
            "Action 1\n",
            "<search>Transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set the question\n",
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences = ['\\nObservation'],\n",
        "    system_instruction = model_instructions + example1 + example2\n",
        "    )\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_response = client.chats.create(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = react_config\n",
        ")\n",
        "\n",
        "resp = react_response.send_message(question)\n",
        "\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOpInq9c5rh"
      },
      "source": [
        "# Thinking Mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VVV2vqUbKvGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "ce48b097-293f-425f-fde8-219bad472d7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The youngest author listed on the Transformer NLP paper, \"Attention is All You Need,\" is **Aidan N. Gomez**.\n\nWhile precise birthdates aren't typically public information for researchers, we can infer relative ages based on their career stage at the time of the paper's publication (2017).\n\n* **Aidan N. Gomez** was a PhD student at the University of Toronto and a visiting researcher at Google Brain when the paper was published. Being a PhD student at the time strongly suggests he was likely the youngest author among the group, which primarily consisted of established researchers at Google Brain.\n\nThe other authors were:\n\n* Ashish Vaswani\n* Noam Shazeer\n* Niki Parmar\n* Jakob Uszkoreit\n* Llion Jones\n* Łukasz Kaiser\n* Illia Polosukhin\n\nThese other authors were all affiliated with Google Brain and likely in more senior research positions at the time, making it highly probable that Aidan N. Gomez, as a PhD student, was the youngest."
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# set a response\n",
        "response_thinking = client.models.generate_content_stream(\n",
        "    model = 'gemini-2.0-flash-thinking-exp',\n",
        "    contents = 'Who was the youngest athor listed on the transofrmer NLP paper'\n",
        ")\n",
        "\n",
        "buffer = io.StringIO()\n",
        "for chunk in response_thinking:\n",
        "  buffer.write(chunk.text)\n",
        "\n",
        "  # display rsponse as it is streamed\n",
        "  print(chunk.text, end = '')\n",
        "\n",
        "# render the finished response as formatted markdown\n",
        "clear_output()\n",
        "Markdown(buffer.getvalue())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Prompting"
      ],
      "metadata": {
        "id": "bU192AlB7k-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "99SPUqzieldE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "366fb402-bdea-4a05-8643-52edeb7fb202"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's the structure for a Python function to calculate the factorial of a number, without providing the actual code:\n\n**1. Function Definition:**\n\n*   Start by defining a function using the `def` keyword.  Give the function a descriptive name like `factorial`.\n*   The function should accept one argument: the number for which you want to calculate the factorial.  Let's call this argument `n`.\n\n**2. Input Validation:**\n\n*   The first step inside the function should be to check if the input `n` is valid. Factorial is typically defined for non-negative integers.  You'll want to handle cases where `n` is:\n    *   Negative: Return an appropriate message or raise an exception to signal an error, since factorial is not defined for negative numbers.\n    *   Not an integer:  You can either truncate it (only if you decide that is an acceptable behavior), raise an error or decide what is appropriate for the usage.\n\n**3. Base Case:**\n\n*   The factorial of 0 is 1.  This is your base case for the recursion (if you choose to use recursion) or a simple condition in an iterative approach.  If `n` is 0, return 1.\n\n**4. Recursive or Iterative Calculation:**\n\n*   **Recursive Approach:**\n    *   If using recursion, the function will call itself.\n    *   The factorial of `n` is `n` multiplied by the factorial of `n-1`. So, the function should return `n * factorial(n-1)`.\n*   **Iterative Approach:**\n    *   If you prefer an iterative approach, you'll use a loop.\n    *   Initialize a variable (e.g., `result`) to 1.\n    *   Loop from 1 to `n` (inclusive).  In each iteration, multiply `result` by the current loop number.\n\n**5. Return Value:**\n\n*   Finally, the function should return the calculated factorial, stored in the `result` variable (if you used the iterative approach) or obtained from the recursive calls.\n\n**In Summary**\n\nThe function needs to:\n\n1.  Define the function with an input argument for the number.\n2.  Validate the input to ensure it's a non-negative integer (or handle other cases per your requirements).\n3.  Handle the base case (factorial of 0 is 1).\n4.  Calculate the factorial either recursively or iteratively.\n5.  Return the calculated factorial.\n"
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# initiate a code prompt\n",
        "code_prompt = \"\"\"\n",
        "write a python function to calculate the factorial of a number.\n",
        "just explain how to write dont provide the code. remember dont write the code,\n",
        "just give a structure\n",
        "\"\"\"\n",
        "\n",
        "response_code = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens=1024\n",
        "    ),\n",
        "    contents = code_prompt\n",
        ")\n",
        "\n",
        "\n",
        "Markdown(response_code.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Execution"
      ],
      "metadata": {
        "id": "QEZoY-8x9zGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a config type\n",
        "config = types.GenerateContentConfig(\n",
        "    tools = [types.Tool(codeExecution=types.ToolCodeExecution())]\n",
        ")\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Generate the first 14 odd prime numbers, then calculate their sum\n",
        "\"\"\"\n",
        "\n",
        "response_exec_code = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = config,\n",
        "    contents = code_exec_prompt\n",
        ")\n",
        "\n",
        "for part in response_exec_code.candidates[0].content.parts:\n",
        "  pprint(part.to_json_dict())\n",
        "  print('------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82P3-rMA8b9d",
        "outputId": "7a843b73-98e9-4ccf-e80f-22b15b89f9b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n",
            "         'prime numbers. Remember that a prime number is a whole number '\n",
            "         'greater than 1 that has only two divisors: 1 and itself. Also, the '\n",
            "         'number 2 is the only even prime number. Since you asked for odd '\n",
            "         'prime numbers, I can skip 2.\\n'\n",
            "         '\\n'\n",
            "         \"Here's how I will proceed:\\n\"\n",
            "         '\\n'\n",
            "         '1.  Start with the first odd prime number, which is 3.\\n'\n",
            "         '2.  Iterate through odd numbers, checking if they are prime.\\n'\n",
            "         '3.  Add the prime numbers to a list until I have 14 of them.\\n'\n",
            "         '4.  Calculate the sum of the prime numbers in the list.\\n'\n",
            "         '\\n'}\n",
            "------------\n",
            "{'executable_code': {'code': 'def is_prime(n):\\n'\n",
            "                             '  \"\"\"Check if a number is prime.\"\"\"\\n'\n",
            "                             '  if n <= 1:\\n'\n",
            "                             '    return False\\n'\n",
            "                             '  for i in range(2, int(n**0.5) + 1):\\n'\n",
            "                             '    if n % i == 0:\\n'\n",
            "                             '      return False\\n'\n",
            "                             '  return True\\n'\n",
            "                             '\\n'\n",
            "                             'primes = []\\n'\n",
            "                             'num = 3\\n'\n",
            "                             'while len(primes) < 14:\\n'\n",
            "                             '  if is_prime(num):\\n'\n",
            "                             '    primes.append(num)\\n'\n",
            "                             '  num += 2\\n'\n",
            "                             '\\n'\n",
            "                             \"print(f'{primes=}')\\n\"\n",
            "                             '\\n'\n",
            "                             'sum_of_primes = sum(primes)\\n'\n",
            "                             \"print(f'{sum_of_primes=}')\\n\",\n",
            "                     'language': 'PYTHON'}}\n",
            "------------\n",
            "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
            "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
            "                                     '31, 37, 41, 43, 47]\\n'\n",
            "                                     'sum_of_primes=326\\n'}}\n",
            "------------\n",
            "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
            "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n",
            "------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WbdRLynw_un9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explaining code"
      ],
      "metadata": {
        "id": "hiUQUH4qfBtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specifiy a path to a file content\n",
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "response_explain_code = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = explain_prompt\n",
        ")\n",
        "\n",
        "Markdown(response_explain_code.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kfxJlZxCfEbd",
        "outputId": "10df4d1e-9065-47b4-fc36-87ee83786898"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a Bash script designed to enhance your command-line prompt with information about the current Git repository.  It's commonly referred to as a \"Git prompt script.\"\n\n**What it does (High-Level):**\n\n*   **Displays Git Status:**  Shows you the current branch, whether there are uncommitted changes (staged, unstaged, untracked), if you're ahead or behind the remote, and other useful Git-related information directly in your terminal prompt.\n*   **Customizable:**  It's highly configurable, allowing you to customize the colors, symbols, and the specific information displayed in the prompt.  You can define your own themes and color schemes.\n*   **Asynchronous Updates (Optional):** Can be configured to fetch remote Git status in the background so the prompt doesn't slow down your workflow.\n\n**Why you would use it:**\n\n*   **At-a-glance Git Information:** Provides immediate feedback about the state of your Git repository without needing to run `git status` constantly. This saves time and reduces the risk of committing changes in the wrong state.\n*   **Improved Workflow:**  Makes it easier to track your Git activity and avoid common mistakes.\n*   **Personalized Prompt:** Allows you to create a prompt that is visually appealing and contains the specific Git information most relevant to you.\n*   **Virtual environment support:** Support to show in the prompt when you are inside a python or node environment.\n\nIn essence, it makes your terminal prompt \"Git-aware\" so you're always informed about your repository's status while working in the command line.\n"
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fV9kXfmrfzH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+4lKhc4qOLDOSr9hlNT66",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}