{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo0hyEMrthXTmHyDU1RWCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirTee12/LLM-Kaggle-Google-GenAI/blob/main/Prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the SDK"
      ],
      "metadata": {
        "id": "QJ85fhKIppqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e7_t8fZxbZMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b022de-aa8d-4708-c84f-6a0e1a559b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab # uninstall jupyterlab in the case it conflict google colab base image\n",
        "!pip install -U -q \"google-genai==1.7.0\" # install google genai library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the SDK and some helpers for rendering the output. The `types` in `import types` may include custom data structures, classes or type hints psecifically to work with google AI. These custom types might represent things like model parameters, input/output formats for AI models or config settings."
      ],
      "metadata": {
        "id": "jkGlMbU9ueF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import enum\n",
        "from IPython.display import HTML, Markdown, display\n",
        "import typing_extensions as typing"
      ],
      "metadata": {
        "id": "vn5P5o2Zp4Jd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Call Retry Implementation\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota\n",
        "\n",
        "\n",
        "1.  **Imports `retry`:** It imports the `retry` module from `google.api_core`, which provides tools for automatic retries of API calls.\n",
        "\n",
        "2.  **Defines `is_retriable`:**\n",
        "    * It creates a lambda function called `is_retriable`.\n",
        "    * This function checks if an exception (`e`) is an instance of `genai.errors.APIErrpr` and if its error code (`e.code`) is either 429 (Too Many Requests) or 503 (Service Unavailable).\n",
        "    * It returns `True` if both conditions are met (meaning the API call is considered retriable), and `False` otherwise.\n",
        "\n",
        "3.  **Applies Retry Logic:**\n",
        "    * It modifies the `genai.models.Models.generate_content` function by wrapping it with the `retry.retry` function.\n",
        "    * The `retry.retry` function is configured with the `is_retriable` function as the `predicate`. This tells `retry.retry` to only retry the API call if the `is_retriable` function returns `True` for the encountered exception.\n",
        "    * Essentially, this replaces the original `generate_content` function with a new version that automatically retries on 429 and 503 errors, making the code more robust to temporary API issues."
      ],
      "metadata": {
        "id": "JCueWvEs3iyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429, 503))\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate = is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "tyJcQ1EW3tec"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"secret-label-prompt\")"
      ],
      "metadata": {
        "id": "8jzS_Om16DxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ceb1f515-75dc-4fd8-b0ee-e1b5e84c93c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kaggle_secrets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-215ab66f97c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msecret_value_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"secret-label-prompt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the API keys and create a client\n",
        "\n",
        "Retrieves the API key securely and create a client object that will be used to communicate with the generative AI service, using the retrieved API key for authentication."
      ],
      "metadata": {
        "id": "qmMX2ZA28UP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # retrive the APi key\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)   # create a client to interact with the genai application"
      ],
      "metadata": {
        "id": "VpfMD3Jo7_IX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the First Prompt"
      ],
      "metadata": {
        "id": "vaCFpBre_3sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a response\n",
        "response = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = \"Explain AI to me like I'm a kid\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "-zXMiooZ_pMB",
        "outputId": "3c670e27-4880-48ea-d293-a4d5d973449f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really, really smart puppy! That puppy can learn tricks, right?  You show it something over and over, and eventually, it learns to do it on its own.\n",
            "\n",
            "AI is like that, but instead of a puppy, it's a computer program.  We teach the computer program how to do things by showing it lots and lots of examples.\n",
            "\n",
            "*   **Learning by Example:** Let's say you want to teach the computer to recognize cats. You show it thousands of pictures of cats. After seeing so many cat pictures, the computer learns what cats generally look like.\n",
            "*   **Making Choices:**  Then, you show the computer a new picture and ask, \"Is this a cat?\" Because it learned from all those examples, it can usually guess correctly!\n",
            "*   **Helping Us Out:** AI can do lots of things like recognize faces in pictures, suggest what movies you might like to watch, or even help doctors find diseases.\n",
            "\n",
            "So basically, AI is a computer program that learns from information and can do smart things like a very well-trained puppy (but without the need to be walked!).\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine you have a really, really smart puppy! That puppy can learn tricks, right?  You show it something over and over, and eventually, it learns to do it on its own.\n\nAI is like that, but instead of a puppy, it's a computer program.  We teach the computer program how to do things by showing it lots and lots of examples.\n\n*   **Learning by Example:** Let's say you want to teach the computer to recognize cats. You show it thousands of pictures of cats. After seeing so many cat pictures, the computer learns what cats generally look like.\n*   **Making Choices:**  Then, you show the computer a new picture and ask, \"Is this a cat?\" Because it learned from all those examples, it can usually guess correctly!\n*   **Helping Us Out:** AI can do lots of things like recognize faces in pictures, suggest what movies you might like to watch, or even help doctors find diseases.\n\nSo basically, AI is a computer program that learns from information and can do smart things like a very well-trained puppy (but without the need to be walked!).\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start a chat"
      ],
      "metadata": {
        "id": "hbQZTsixG3hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model = \"gemini-2.0-flash\", history = [])\n",
        "response = chat.send_message('Hello, my name is Ahmad')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvlI-MyAZNj",
        "outputId": "68bf292a-688e-4548-a571-24cbb9cacc8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Ahmad! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"can you tell me something interesting about dinosaurs\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZR6L3fHTEP",
        "outputId": "b90626ed-951c-40b9-8dde-30e8048ed347"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a fascinating fact about dinosaurs:\n",
            "\n",
            "**The Spinosaurus was likely the first known swimming dinosaur.**\n",
            "\n",
            "For a long time, dinosaurs were thought to be strictly land-based creatures. However, discoveries in recent years have strongly suggested that the Spinosaurus, a massive theropod dinosaur (larger than a T-Rex), spent a significant amount of its time in the water. Evidence for this includes:\n",
            "\n",
            "*   **Its dense bones:** Unlike most theropods which had hollow bones for lightness, Spinosaurus had dense bones, which would have helped it stay submerged.\n",
            "*   **Its paddle-like feet:** These feet were more suited for paddling than walking on land.\n",
            "*   **Its sail-like structure:** Scientists now believe that the large sail on its back might have been used for stability in the water or for display.\n",
            "*   **Isotope analysis of its teeth:** Showed similar oxygen isotope ratios to aquatic animals living in the same area.\n",
            "*   **Fossils found in aquatic environments:** Many Spinosaurus fossils have been found in locations that were once river systems.\n",
            "*   **Tail Structure:** Evidence suggests the tail of Spinosaurus was more akin to that of a crocodile, and aided in propulsion.\n",
            "\n",
            "So, the Spinosaurus challenges our traditional image of dinosaurs and suggests that some of them were much more adapted to aquatic life than we previously thought!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"do you still remember my name\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1O0TSqHef2",
        "outputId": "6d8b2a45-1666-4f25-f42a-6f25acf287a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Ahmad! I remember your name.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chose a model from the Gemini model family\n",
        "\n",
        "Retrieves a list of available AI models, searches for a specific model named \"gemini-2.0-flash,\" and if found, displays its details in a formatted JSON-like structure. The search stops once the target model is located."
      ],
      "metadata": {
        "id": "uUBrfruvh7mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "id": "CmeO6A6YIe0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3dd1a5-2d21-4693-c524-de5f495a84ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters Exploration"
      ],
      "metadata": {
        "id": "Fi_3neIHkzaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Length"
      ],
      "metadata": {
        "id": "MGm-Pg5OmE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the max number of output token to 200\n",
        "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
        "\n",
        "# create a response\n",
        "response_short = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = short_config,\n",
        "    contents='write a short essay on the importance of education in modern day society.'\n",
        ")\n",
        "\n",
        "print(response_short.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpL4Sb2oiu7r",
        "outputId": "bef2b14c-1788-4f85-e7c4-281b1a8fe394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Cornerstone of Progress: Education in Modern Society\n",
            "\n",
            "In the swirling currents of modern society, a society defined by rapid technological advancements, complex global challenges, and ever-shifting social landscapes, education stands as the unwavering cornerstone of progress. No longer simply the acquisition of knowledge, it is the essential tool that empowers individuals, strengthens communities, and shapes a future brimming with possibility. Its importance transcends mere career prospects; it is fundamental to individual empowerment, economic growth, and societal well-being.\n",
            "\n",
            "Firstly, education empowers individuals by equipping them with critical thinking skills. In an age saturated with information and misinformation, the ability to analyze, evaluate, and synthesize knowledge is paramount. Education fosters this intellectual independence, enabling individuals to navigate the complexities of the modern world and make informed decisions. It encourages questioning, exploration, and the development of unique perspectives, fostering creativity and innovation that are vital for tackling the challenges ahead. Furthermore, education promotes self-awareness and understanding, leading to increased confidence and a greater sense of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "How much randomness is included when selecting the next word (token) in language models depends on the \"temperature\" parameter.  A higher temperature causes the model to take into account a greater number of potential words, producing more inventive and diverse results.  On the other hand, a lower temperature forces the model to adhere to the most likely terms, producing language that is more concentrated and predictable.  The model turns completely deterministic when the temperature is set to 0, always choosing the word that is the most likely.  Temperature does not, however, ensure actual randomness; rather, it is a means of directing the model toward more or less random results."
      ],
      "metadata": {
        "id": "ErIkZR5snjVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVK4xyumIuX",
        "outputId": "8a33e602-77c0-43a9-f019-0f1b5ea86799"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magenta\n",
            " _________________________\n",
            "Orange\n",
            " _________________________\n",
            "Azure.\n",
            " _________________________\n",
            "Magenta\n",
            " _________________________\n",
            "Turquoise\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous output shows the randomness but we would experiment with temperature value of 0. we can see from the output that it is more direct and precise and there is no randomness."
      ],
      "metadata": {
        "id": "_zRPP_mrs-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPCPP9Mox8R",
        "outputId": "28d56a09-7f08-4a58-ca7b-8064bb85d7c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top P"
      ],
      "metadata": {
        "id": "8Y-BK9xPuN9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 1.0, # default config\n",
        "    top_p = 0.95       # default config\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a senior network engineer. Explain ospf to a 5 year old\"\n",
        "response_topp = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents=story_prompt\n",
        ")\n",
        "\n",
        "print(response_topp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9LE2k_7t098",
        "outputId": "e74ccbd4-c3b1-415e-99df-44dc353a421d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine the internet is like a giant neighborhood filled with lots of houses (computers). Each house needs to know how to get to every other house.  \n",
            "\n",
            "Now, OSPF is like a super-smart mailman!\n",
            "\n",
            "*   **Everyone knows everyone:** First, all the houses that want to talk to each other get together and make a map of the entire neighborhood. This map shows all the streets (network connections) and houses (computers).\n",
            "\n",
            "*   **Finding the best path:** Instead of just picking any road, OSPF figures out the *fastest* and *easiest* way to deliver mail (data). It's like finding the shortest route with the least traffic!\n",
            "\n",
            "*   **If a road closes:** If a street gets blocked (the connection breaks), OSPF is super quick to update the map. All the houses get a new map, so they know to take a different route.\n",
            "\n",
            "*   **Talking to neighbors:** Houses don't need to yell across the entire neighborhood. They only need to talk to their neighbors (other computers connected to them). This makes things much faster and less noisy!\n",
            "\n",
            "So, OSPF helps all the houses on the internet find the best and fastest way to send messages to each other, even if some roads get blocked! It's a very clever way to deliver messages efficiently.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting"
      ],
      "metadata": {
        "id": "A8rqMwXfHN3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot Prompt"
      ],
      "metadata": {
        "id": "aZLlvtk0HRao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the prompt parameters\n",
        "model_config = types.GenerateContentConfig(\n",
        "  temperature = 0.1,\n",
        "  top_p = 1,\n",
        "  max_output_tokens = 5\n",
        ")\n",
        "\n",
        "# set the prompt\n",
        "zero_short_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response_zero_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "\n",
        "print(response_zero_shot.text) # print the output as text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BjVXOH_m7Y",
        "outputId": "465c4d58-475e-43d9-c6b4-37b920efe4f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enum Mode\n",
        "The Sentiment enum acts as a schema or a constraint. It tells the language model that the expected output should be one of these three predefined sentiment categories.\n",
        "\n",
        "The `text/x.enum` MIME type indicates that the response should be one of the values defined in the response_schema."
      ],
      "metadata": {
        "id": "B1qz1KnEKK5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new enumeration class tha inherit from enum.Enum\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'positive'\n",
        "  NEUTRAL = 'neutral'\n",
        "  NEGATIVE = 'negative'\n",
        "\n",
        "response_zero_short_enum = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "\n",
        "        # tell the model the ezpected output format and that the rsponse should\n",
        "        # be one of thos defined in the schema\n",
        "        response_mime_type = 'text/x.enum',\n",
        "\n",
        "        # set the Sentiment as schema\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "print(response_zero_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iJTirTJpqN",
        "outputId": "d9eca068-ef77-4caf-80f5-da8a1ff4b8bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Short Prompt"
      ],
      "metadata": {
        "id": "3mVeGOaOc4l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = 'Give me a large with cheese and pineapple'\n",
        "\n",
        "response_one_few_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response_one_few_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UOTJ-HQojw",
        "outputId": "32f52b43-a152-4136-d2ce-7fc096482771"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JSON Mode\n",
        "\n",
        "`class PizzaOrder(typing.TypedDict):`: This defines a new class called PizzaOrder that inherits from typing.TypedDict. TypedDict is used to create dictionary types where the keys and their corresponding value types are known\n",
        "\n",
        "Purpose of the `TypedDict` is to act as a schema. It defines the structure and data types of the JSON output we expect from the language model. This allows us to work with the model's response in a structured and predictable way.\n"
      ],
      "metadata": {
        "id": "E0OeKzzZ4-S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PizzaOrder(typing.TypedDict):\n",
        "  size:str\n",
        "  ingredients: list[str]\n",
        "  type:str\n",
        "\n",
        "response_json = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        response_mime_type = 'application/json',\n",
        "        response_schema = PizzaOrder\n",
        "    ),\n",
        "\n",
        "    contents = \"Can I have a large dessert pizza with apple and chocolate\"\n",
        ")\n",
        "\n",
        "print(response_json.text)"
      ],
      "metadata": {
        "id": "pJ-0mRg3d4L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7494c294-348b-4210-f59c-1d834acdf82c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought\n",
        "\n",
        "Chain of Thought prompting improves the language model's ability to perform complex reasoning tasks. By explicitly requesting the model to show its reasoning process, it's more likely to arrive at the correct answer, as it avoids impulsive or incorrect conclusions."
      ],
      "metadata": {
        "id": "UCwQgelU1eXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the prompt\n",
        "cot_prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? let's think step by step.\"\"\"\n",
        "\n",
        "response_cot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = cot_prompt)\n",
        "\n",
        "Markdown(response_cot.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "2QOblph7-QL1",
        "outputId": "a8eb466b-8621-4e89-a7a1-a2fa34814847"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Determine partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Now that you are 20, your partner is 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**\n"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwSgOshf2BqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}