{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirTee12/LLM-Kaggle-Google-GenAI/blob/main/Prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ85fhKIppqo"
      },
      "source": [
        "## Install the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7_t8fZxbZMh",
        "outputId": "93d95179-aa6a-4c11-b9bd-b7483d82050d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab # uninstall jupyterlab in the case it conflict google colab base image\n",
        "!pip install -U -q \"google-genai==1.7.0\" # install google genai library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkGlMbU9ueF5"
      },
      "source": [
        "import the SDK and some helpers for rendering the output. The `types` in `import types` may include custom data structures, classes or type hints psecifically to work with google AI. These custom types might represent things like model parameters, input/output formats for AI models or config settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn5P5o2Zp4Jd"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import enum\n",
        "import io\n",
        "from IPython.display import HTML, Markdown, display, clear_output\n",
        "import typing_extensions as typing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCueWvEs3iyF"
      },
      "source": [
        "## API Call Retry Implementation\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota\n",
        "\n",
        "\n",
        "1.  **Imports `retry`:** It imports the `retry` module from `google.api_core`, which provides tools for automatic retries of API calls.\n",
        "\n",
        "2.  **Defines `is_retriable`:**\n",
        "    * It creates a lambda function called `is_retriable`.\n",
        "    * This function checks if an exception (`e`) is an instance of `genai.errors.APIErrpr` and if its error code (`e.code`) is either 429 (Too Many Requests) or 503 (Service Unavailable).\n",
        "    * It returns `True` if both conditions are met (meaning the API call is considered retriable), and `False` otherwise.\n",
        "\n",
        "3.  **Applies Retry Logic:**\n",
        "    * It modifies the `genai.models.Models.generate_content` function by wrapping it with the `retry.retry` function.\n",
        "    * The `retry.retry` function is configured with the `is_retriable` function as the `predicate`. This tells `retry.retry` to only retry the API call if the `is_retriable` function returns `True` for the encountered exception.\n",
        "    * Essentially, this replaces the original `generate_content` function with a new version that automatically retries on 429 and 503 errors, making the code more robust to temporary API issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyJcQ1EW3tec"
      },
      "outputs": [],
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429, 503))\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate = is_retriable)(genai.models.Models.generate_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jzS_Om16DxV"
      },
      "outputs": [],
      "source": [
        "#from kaggle_secrets import UserSecretsClient\n",
        "#user_secrets = UserSecretsClient()\n",
        "#secret_value_0 = user_secrets.get_secret(\"secret-label-prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmMX2ZA28UP_"
      },
      "source": [
        "## Get the API keys and create a client\n",
        "\n",
        "Retrieves the API key securely and create a client object that will be used to communicate with the generative AI service, using the retrieved API key for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpfMD3Jo7_IX"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # retrive the APi key\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)   # create a client to interact with the genai application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaCFpBre_3sF"
      },
      "source": [
        "## Running the First Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "-zXMiooZ_pMB",
        "outputId": "21f9650a-6d17-41d7-8dad-f8e74e785f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, imagine you have a really, really smart puppy! That puppy is like AI.\n",
            "\n",
            "Instead of fur and a tail, AI is made of computer programs. These programs are like instructions that tell the puppy (AI) what to do.\n",
            "\n",
            "**Here's how it works:**\n",
            "\n",
            "*   **Learning:** Just like you teach a puppy to sit by giving it treats when it sits, we teach AI by giving it lots and lots of information. For example, if we want AI to recognize cats, we show it thousands of pictures of cats!\n",
            "\n",
            "*   **Thinking:** After seeing all those cats, the AI puppy starts to learn what makes a cat a cat - pointy ears, whiskers, a tail, etc. It can then \"think\" and recognize new cats it hasn't seen before!\n",
            "\n",
            "*   **Doing Things:**  So, instead of just sitting, AI can do lots of things like:\n",
            "\n",
            "    *   Help you find videos you like to watch.\n",
            "    *   Answer questions you have.\n",
            "    *   Play games with you.\n",
            "    *   Even help doctors find illnesses!\n",
            "\n",
            "**But AI isn't really a *real* puppy. It doesn't have feelings or understand things the way you do.** It's just really good at following instructions and learning patterns.\n",
            "\n",
            "**Important things to remember:**\n",
            "\n",
            "*   **AI is a tool.**  Like a hammer or a crayon, it helps us do things.\n",
            "*   **People make AI.** We teach it and tell it what to do.\n",
            "*   **AI is getting smarter all the time!**\n",
            "\n",
            "So, next time you hear about AI, think of that super-smart computer puppy that's learning and helping people!\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "Okay, imagine you have a really, really smart puppy! That puppy is like AI.\n\nInstead of fur and a tail, AI is made of computer programs. These programs are like instructions that tell the puppy (AI) what to do.\n\n**Here's how it works:**\n\n*   **Learning:** Just like you teach a puppy to sit by giving it treats when it sits, we teach AI by giving it lots and lots of information. For example, if we want AI to recognize cats, we show it thousands of pictures of cats!\n\n*   **Thinking:** After seeing all those cats, the AI puppy starts to learn what makes a cat a cat - pointy ears, whiskers, a tail, etc. It can then \"think\" and recognize new cats it hasn't seen before!\n\n*   **Doing Things:**  So, instead of just sitting, AI can do lots of things like:\n\n    *   Help you find videos you like to watch.\n    *   Answer questions you have.\n    *   Play games with you.\n    *   Even help doctors find illnesses!\n\n**But AI isn't really a *real* puppy. It doesn't have feelings or understand things the way you do.** It's just really good at following instructions and learning patterns.\n\n**Important things to remember:**\n\n*   **AI is a tool.**  Like a hammer or a crayon, it helps us do things.\n*   **People make AI.** We teach it and tell it what to do.\n*   **AI is getting smarter all the time!**\n\nSo, next time you hear about AI, think of that super-smart computer puppy that's learning and helping people!\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate a response\n",
        "response = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = \"Explain AI to me like I'm a kid\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQZTsixG3hl"
      },
      "source": [
        "## Let's start a chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvlI-MyAZNj",
        "outputId": "c24557bd-bff8-46a6-9ef7-d04c34aac9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello Ahmad! It's nice to meet you. How can I help you today?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat = client.chats.create(model = \"gemini-2.0-flash\", history = [])\n",
        "response = chat.send_message('Hello, my name is Ahmad')\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZR6L3fHTEP",
        "outputId": "d65d300c-f09e-4153-fc39-fc9742ae2121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, here's an interesting fact about dinosaurs that might surprise you:\n",
            "\n",
            "**Birds are actually the direct descendants of small, feathered theropod dinosaurs.**\n",
            "\n",
            "That's right, the chickens in your backyard are more closely related to a Tyrannosaurus Rex than a Stegosaurus is!\n",
            "\n",
            "Here's why this is so fascinating:\n",
            "\n",
            "*   **Fossil Evidence:** We've found tons of fossils showing feathered dinosaurs. These weren't just proto-feathers for insulation, but complex flight feathers.\n",
            "*   **Skeletal Similarities:** Bird skeletons and the skeletons of theropod dinosaurs (like Velociraptors) share many features, including hollow bones, a three-fingered hand, and a wishbone (furcula).\n",
            "*   **Behavioral Clues:** Fossil discoveries suggest that some dinosaurs even incubated their eggs like modern birds.\n",
            "\n",
            "So, next time you see a bird, remember you're looking at a living dinosaur!\n",
            "\n",
            "Is there anything else you'd like to know about dinosaurs? Maybe something specific about a particular type of dinosaur, or a specific period in history?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"can you tell me something interesting about dinosaurs\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1O0TSqHef2",
        "outputId": "14b7634a-391c-4030-8549-74ae20a52f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, I remember your name is Ahmad. I am designed to remember details from our conversation.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"do you still remember my name\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUBrfruvh7mS"
      },
      "source": [
        "# Chose a model from the Gemini model family\n",
        "\n",
        "Retrieves a list of available AI models, searches for a specific model named \"gemini-2.0-flash,\" and if found, displays its details in a formatted JSON-like structure. The search stops once the target model is located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkU0vWoSglap",
        "outputId": "b236b399-7508-4f4e-e63d-cf196a30c661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='models/chat-bison-001' display_name='PaLM 2 Chat (Legacy)' description='A legacy text-only model optimized for chat conversations' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=4096 output_token_limit=1024 supported_actions=['generateMessage', 'countMessageTokens']\n",
            "name='models/text-bison-001' display_name='PaLM 2 (Legacy)' description='A legacy model that understands text and generates text as an output' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8196 output_token_limit=1024 supported_actions=['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens']\n",
            "name='models/gemini-1.0-pro-vision-latest' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-pro-vision' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-latest' display_name='Gemini 1.5 Pro Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-001' display_name='Gemini 1.5 Pro 001' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro-002' display_name='Gemini 1.5 Pro 002' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro' display_name='Gemini 1.5 Pro' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-latest' display_name='Gemini 1.5 Flash Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-001' display_name='Gemini 1.5 Flash 001' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-001-tuning' display_name='Gemini 1.5 Flash 001 Tuning' description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=16384 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createTunedModel']\n",
            "name='models/gemini-1.5-flash' display_name='Gemini 1.5 Flash' description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-002' display_name='Gemini 1.5 Flash 002' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-8b' display_name='Gemini 1.5 Flash-8B' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-001' display_name='Gemini 1.5 Flash-8B 001' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-latest' display_name='Gemini 1.5 Flash-8B Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0827' display_name='Gemini 1.5 Flash 8B Experimental 0827' description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0924' display_name='Gemini 1.5 Flash 8B Experimental 0924' description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.5-pro-exp-03-25' display_name='Gemini 2.5 Pro Experimental 03-25' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.5-pro-preview-03-25' display_name='Gemini 2.5 Pro Preview 03-25' description='Gemini 2.5 Pro Preview 03-25' version='2.5-preview-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-exp-image-generation' display_name='Gemini 2.0 Flash (Image Generation) Experimental' description='Gemini 2.0 Flash (Image Generation) Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.0 Flash Thinking Experimental' description='Gemini 2.0 Flash Thinking Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/learnlm-1.5-pro-experimental' display_name='LearnLM 1.5 Pro Experimental' description='Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32767 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/learnlm-2.0-flash-experimental' display_name='LearnLM 2.0 Flash Experimental' description='LearnLM 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=32768 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer']\n",
            "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0 002 model' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predict']\n",
            "name='models/gemini-2.0-flash-live-001' display_name='Gemini 2.0 Flash 001' description='Gemini 2.0 Flash 001' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['bidiGenerateContent', 'countTokens']\n"
          ]
        }
      ],
      "source": [
        "for model in client.models.list():\n",
        "  print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmeO6A6YIe0E",
        "outputId": "db034cd3-feea-4a53-c6f2-fa7b41d8c49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens', 'createCachedContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi_3neIHkzaj"
      },
      "source": [
        "## Parameters Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGm-Pg5OmE3Z"
      },
      "source": [
        "### Output Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpL4Sb2oiu7r",
        "outputId": "c44311f2-28e8-4e39-946e-05adbd6cf89c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## The Cornerstone of Progress: Why Education Remains Vital in Modern Society\n",
            "\n",
            "In the rapidly evolving landscape of the 21st century, education stands as a more critical pillar than ever before. It is not merely the acquisition of facts and figures, but a dynamic process that equips individuals with the skills, knowledge, and critical thinking abilities necessary to navigate the complexities of modern life and contribute meaningfully to a globalized society. The importance of education in today's world extends far beyond personal enrichment, serving as the cornerstone for economic growth, social progress, and the advancement of human understanding.\n",
            "\n",
            "Firstly, education is undeniably linked to economic prosperity, both at the individual and national levels. In a knowledge-based economy, where technological innovation and adaptability are paramount, individuals with higher levels of education are more likely to secure stable, well-paying jobs. They possess the skills necessary to adapt to evolving industries, embrace new technologies, and contribute to innovation. Furthermore, a well-educated workforce fosters economic growth by attracting\n"
          ]
        }
      ],
      "source": [
        "# set the max number of output token to 200\n",
        "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
        "\n",
        "# create a response\n",
        "response_short = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = short_config,\n",
        "    contents='write a short essay on the importance of education in modern day society.'\n",
        ")\n",
        "\n",
        "print(response_short.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErIkZR5snjVC"
      },
      "source": [
        "### Temperature\n",
        "\n",
        "How much randomness is included when selecting the next word (token) in language models depends on the \"temperature\" parameter.  A higher temperature causes the model to take into account a greater number of potential words, producing more inventive and diverse results.  On the other hand, a lower temperature forces the model to adhere to the most likely terms, producing language that is more concentrated and predictable.  The model turns completely deterministic when the temperature is set to 0, always choosing the word that is the most likely.  Temperature does not, however, ensure actual randomness; rather, it is a means of directing the model toward more or less random results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVK4xyumIuX",
        "outputId": "c4f50ca9-38db-41a8-9726-90e7688ea7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Magenta\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Magenta.\n",
            " _________________________\n",
            "Cerulean.\n",
            " _________________________\n",
            "Orange\n",
            " _________________________\n"
          ]
        }
      ],
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zRPP_mrs-1z"
      },
      "source": [
        "The previous output shows the randomness but we would experiment with temperature value of 0. we can see from the output that it is more direct and precise and there is no randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPCPP9Mox8R",
        "outputId": "13142944-cb28-46f7-e1b3-dd9efe930771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ],
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y-BK9xPuN9X"
      },
      "source": [
        "### Top P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9LE2k_7t098",
        "outputId": "dd6b0d7e-0bc8-4ea4-bca3-675f9f21f0e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, imagine we're all delivery trucks trying to deliver packages to different houses in our neighborhood.\n",
            "\n",
            "*   **Houses are like computers on the internet.** Each house needs to get its packages.\n",
            "\n",
            "*   **We, the delivery trucks, are like routers.** Routers are like traffic controllers in the internet.\n",
            "\n",
            "*   **OSPF is like a secret way the delivery trucks talk to each other.** We want to find the *fastest* way to deliver all the packages.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1.  **Whispering Secrets:** Each delivery truck (router) tells its nearby truck friends, \"Hey, I can deliver packages to these houses!\" It's like sharing a little secret.\n",
            "\n",
            "2.  **Drawing a Map:** All the delivery trucks share those secrets with each other. Then, everyone draws a map of the whole neighborhood, showing which trucks can get to which houses.\n",
            "\n",
            "3.  **Finding the Best Road:** Now, if a truck needs to send a package to a house far away, it looks at its map and chooses the *best* road. The \"best\" road might be the shortest, or maybe the one with the fewest cars (other delivery trucks).\n",
            "\n",
            "4.  **Following the Directions:** The truck gives the package to the next truck along the best road. That truck looks at the map too, and knows who to give it to next!\n",
            "\n",
            "5.  **Everyone Knows the Way:** All the trucks use the same map, so everyone knows the fastest way to get the packages delivered to all the houses.\n",
            "\n",
            "So, OSPF helps all the routers (delivery trucks) talk to each other and figure out the best and fastest way to send information (packages) around the internet (neighborhood)! It's like teamwork to make sure everything gets where it needs to go.\n",
            "\n",
            "**Important for a 5-year-old:** Don't get caught up in technical jargon like \"Link State Advertisements.\" Keep it high level and focused on the analogy. The key is to explain the basic concept of information sharing and path optimization in a relatable way.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 1.0, # default config\n",
        "    top_p = 0.95       # default config\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a senior network engineer. Explain ospf to a 5 year old\"\n",
        "response_topp = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents=story_prompt\n",
        ")\n",
        "\n",
        "print(response_topp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8rqMwXfHN3_"
      },
      "source": [
        "## Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZLlvtk0HRao"
      },
      "source": [
        "### Zero-shot Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BjVXOH_m7Y",
        "outputId": "60e22336-3cf9-4ed3-ea5f-98a68eb8e8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set the prompt parameters\n",
        "model_config = types.GenerateContentConfig(\n",
        "  temperature = 0.1,\n",
        "  top_p = 1,\n",
        "  max_output_tokens = 5\n",
        ")\n",
        "\n",
        "# set the prompt\n",
        "zero_short_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response_zero_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "\n",
        "print(response_zero_shot.text) # print the output as text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1qz1KnEKK5c"
      },
      "source": [
        "#### Enum Mode\n",
        "The Sentiment enum acts as a schema or a constraint. It tells the language model that the expected output should be one of these three predefined sentiment categories.\n",
        "\n",
        "The `text/x.enum` MIME type indicates that the response should be one of the values defined in the response_schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iJTirTJpqN",
        "outputId": "609389eb-2a8e-4b0f-86a6-96f961170303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create a new enumeration class tha inherit from enum.Enum\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'positive'\n",
        "  NEUTRAL = 'neutral'\n",
        "  NEGATIVE = 'negative'\n",
        "\n",
        "response_zero_short_enum = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "\n",
        "        # tell the model the ezpected output format and that the rsponse should\n",
        "        # be one of thos defined in the schema\n",
        "        response_mime_type = 'text/x.enum',\n",
        "\n",
        "        # set the Sentiment as schema\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "print(response_zero_shot.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mVeGOaOc4l1"
      },
      "source": [
        "### Few Short Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UOTJ-HQojw",
        "outputId": "92dd0e6d-b894-45eb-fee6-656743b01701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = 'Give me a large with cheese and pineapple'\n",
        "\n",
        "response_one_few_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response_one_few_shot.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0OeKzzZ4-S-"
      },
      "source": [
        "#### JSON Mode\n",
        "\n",
        "`class PizzaOrder(typing.TypedDict):`: This defines a new class called PizzaOrder that inherits from typing.TypedDict. TypedDict is used to create dictionary types where the keys and their corresponding value types are known\n",
        "\n",
        "Purpose of the `TypedDict` is to act as a schema. It defines the structure and data types of the JSON output we expect from the language model. This allows us to work with the model's response in a structured and predictable way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ-0mRg3d4L3",
        "outputId": "ffc2df7b-3f48-4e17-e582-f727b98ef0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "class PizzaOrder(typing.TypedDict):\n",
        "  size:str\n",
        "  ingredients: list[str]\n",
        "  type:str\n",
        "\n",
        "response_json = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        response_mime_type = 'application/json',\n",
        "        response_schema = PizzaOrder\n",
        "    ),\n",
        "\n",
        "    contents = \"Can I have a large dessert pizza with apple and chocolate\"\n",
        ")\n",
        "\n",
        "print(response_json.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCwQgelU1eXp"
      },
      "source": [
        "## Chain of Thought\n",
        "\n",
        "Chain of Thought prompting improves the language model's ability to perform complex reasoning tasks. By explicitly requesting the model to show its reasoning process, it's more likely to arrive at the correct answer, as it avoids impulsive or incorrect conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "2QOblph7-QL1",
        "outputId": "35995a18-3fad-4276-9293-a26343896580"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3.  **Determine the partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set up the prompt\n",
        "cot_prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? let's think step by step.\"\"\"\n",
        "\n",
        "response_cot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = cot_prompt)\n",
        "\n",
        "Markdown(response_cot.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxssztRDiJx"
      },
      "source": [
        "## Reason and Act: ReAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwSgOshf2BqE"
      },
      "outputs": [],
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd-5HwoKEhWu",
        "outputId": "141786ff-7d5f-4c4d-b927-c462bc3229e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thought 1\n",
            "I need to find the transformers NLP paper and identify the youngest author listed.\n",
            "\n",
            "Action 1\n",
            "<search>transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# set the question\n",
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences = ['\\nObservation'],\n",
        "    system_instruction = model_instructions + example1 + example2\n",
        "    )\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_response = client.chats.create(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = react_config\n",
        ")\n",
        "\n",
        "resp = react_response.send_message(question)\n",
        "\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOpInq9c5rh"
      },
      "source": [
        "# Thinking Mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVV2vqUbKvGW"
      },
      "outputs": [],
      "source": [
        "# set a response\n",
        "response_thinking = client.models.generate_content_stream(\n",
        "    model = 'gemini-2.0-flash-thinking-exp',\n",
        "    contents = 'Who was the youngest athor listed on the transofrmer NLP paper'\n",
        ")\n",
        "\n",
        "buffer = io.StringIO()\n",
        "for chunk in response_thinking:\n",
        "  buffer.write(chunk.text)\n",
        "\n",
        "  # display rsponse as it is streamed\n",
        "  print(chunk.text, end = '')\n",
        "\n",
        "# render the finished response as formatted markdown\n",
        "clear_output()\n",
        "Markdown(buffer.getvalue())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Prompting"
      ],
      "metadata": {
        "id": "bU192AlB7k-Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "99SPUqzieldE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "aec1b823-3efa-4178-9c35-f268c22703e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's how you'd structure a Python function to calculate the factorial of a number:\n\n1.  **Define the Function:**\n    -   Start by using the `def` keyword to define your function.\n    -   Give your function a descriptive name, like `factorial`.\n    -   It should accept one argument, which will be the number you want to calculate the factorial of (e.g., `n`).\n\n2.  **Handle Base Cases:**\n    -   Factorial is typically defined recursively, so you need base cases to stop the recursion or the loop.\n    -   The base cases are:\n        -   If `n` is 0, the factorial is 1.\n        -   If `n` is 1, the factorial is 1.  (Often you only explicitly check for n == 0, which covers n == 1 as well in the context of a factorial calculation).\n\n3.  **Recursive (or Iterative) Calculation:**\n    -   **Recursive Approach:**\n        -   If `n` is greater than 1, recursively call the `factorial` function itself, but pass `n - 1` as the argument.\n        -   Multiply the current value of `n` with the result of that recursive call.  This is `n * factorial(n-1)`.\n    -   **Iterative Approach:**\n        -   Initialize a variable (e.g., `result`) to 1.\n        -   Use a loop (e.g., a `for` loop or a `while` loop) that iterates from 2 to `n` (inclusive).\n        -   In each iteration, multiply the `result` by the current number in the loop.\n\n4.  **Return the Result:**\n    -   After the base case is met or the loop completes, return the calculated factorial value.\n"
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# initiate a code prompt\n",
        "code_prompt = \"\"\"\n",
        "write a python function to calculate the factorial of a number.\n",
        "just explain how to write dont provide the code. remember dont write the code,\n",
        "just give a structure\n",
        "\"\"\"\n",
        "\n",
        "response_code = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens=1024\n",
        "    ),\n",
        "    contents = code_prompt\n",
        ")\n",
        "\n",
        "\n",
        "Markdown(response_code.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QEZoY-8x9zGr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82P3-rMA8b9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUOVbi5VHjZdeMT28G3opV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}