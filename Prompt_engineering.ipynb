{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7SmnalmggVzUjt9GKcdze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirTee12/LLM-Kaggle-Google-GenAI/blob/main/Prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the SDK"
      ],
      "metadata": {
        "id": "QJ85fhKIppqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7_t8fZxbZMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d95179-aa6a-4c11-b9bd-b7483d82050d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab # uninstall jupyterlab in the case it conflict google colab base image\n",
        "!pip install -U -q \"google-genai==1.7.0\" # install google genai library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the SDK and some helpers for rendering the output. The `types` in `import types` may include custom data structures, classes or type hints psecifically to work with google AI. These custom types might represent things like model parameters, input/output formats for AI models or config settings."
      ],
      "metadata": {
        "id": "jkGlMbU9ueF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import enum\n",
        "import io\n",
        "from IPython.display import HTML, Markdown, display, clear_output\n",
        "import typing_extensions as typing"
      ],
      "metadata": {
        "id": "vn5P5o2Zp4Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Call Retry Implementation\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota\n",
        "\n",
        "\n",
        "1.  **Imports `retry`:** It imports the `retry` module from `google.api_core`, which provides tools for automatic retries of API calls.\n",
        "\n",
        "2.  **Defines `is_retriable`:**\n",
        "    * It creates a lambda function called `is_retriable`.\n",
        "    * This function checks if an exception (`e`) is an instance of `genai.errors.APIErrpr` and if its error code (`e.code`) is either 429 (Too Many Requests) or 503 (Service Unavailable).\n",
        "    * It returns `True` if both conditions are met (meaning the API call is considered retriable), and `False` otherwise.\n",
        "\n",
        "3.  **Applies Retry Logic:**\n",
        "    * It modifies the `genai.models.Models.generate_content` function by wrapping it with the `retry.retry` function.\n",
        "    * The `retry.retry` function is configured with the `is_retriable` function as the `predicate`. This tells `retry.retry` to only retry the API call if the `is_retriable` function returns `True` for the encountered exception.\n",
        "    * Essentially, this replaces the original `generate_content` function with a new version that automatically retries on 429 and 503 errors, making the code more robust to temporary API issues."
      ],
      "metadata": {
        "id": "JCueWvEs3iyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429, 503))\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate = is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "tyJcQ1EW3tec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from kaggle_secrets import UserSecretsClient\n",
        "#user_secrets = UserSecretsClient()\n",
        "#secret_value_0 = user_secrets.get_secret(\"secret-label-prompt\")"
      ],
      "metadata": {
        "id": "8jzS_Om16DxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the API keys and create a client\n",
        "\n",
        "Retrieves the API key securely and create a client object that will be used to communicate with the generative AI service, using the retrieved API key for authentication."
      ],
      "metadata": {
        "id": "qmMX2ZA28UP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # retrive the APi key\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)   # create a client to interact with the genai application"
      ],
      "metadata": {
        "id": "VpfMD3Jo7_IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the First Prompt"
      ],
      "metadata": {
        "id": "vaCFpBre_3sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a response\n",
        "response = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = \"Explain AI to me like I'm a kid\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "-zXMiooZ_pMB",
        "outputId": "fd8b5d91-4596-4b74-888c-8a68a19a0008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really smart puppy.  This puppy can learn tricks if you show it a lot, and a lot, and a lot of examples.  \n",
            "\n",
            "AI is like that puppy, but instead of learning tricks, it learns to do other things.  \n",
            "\n",
            "For example:\n",
            "\n",
            "*   **Recognize pictures:** You can show it a million pictures of cats and dogs, and eventually, it will learn to tell the difference between them really well! It's like teaching the puppy \"this is a cat!\" and \"this is a dog!\" over and over again.\n",
            "*   **Play games:** You can let it play a video game a million times, and it will learn how to play super well, maybe even better than you! It's like the puppy learning how to fetch the ball perfectly.\n",
            "*   **Talk to you:** You can give it lots and lots of sentences, and it can learn to talk back to you, and even answer your questions.\n",
            "\n",
            "The more examples you give the AI, the better it gets. It's not *really* thinking like you or me, but it's really good at finding patterns and doing what it's been trained to do.\n",
            "\n",
            "**Important:** AI is made by people. So, it's important to use it wisely and make sure it's fair to everyone!  Just like you need to train your puppy kindly, AI needs to be made and used with kindness too.\n",
            "\n",
            "Does that make sense?\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, imagine you have a really smart puppy.  This puppy can learn tricks if you show it a lot, and a lot, and a lot of examples.  \n\nAI is like that puppy, but instead of learning tricks, it learns to do other things.  \n\nFor example:\n\n*   **Recognize pictures:** You can show it a million pictures of cats and dogs, and eventually, it will learn to tell the difference between them really well! It's like teaching the puppy \"this is a cat!\" and \"this is a dog!\" over and over again.\n*   **Play games:** You can let it play a video game a million times, and it will learn how to play super well, maybe even better than you! It's like the puppy learning how to fetch the ball perfectly.\n*   **Talk to you:** You can give it lots and lots of sentences, and it can learn to talk back to you, and even answer your questions.\n\nThe more examples you give the AI, the better it gets. It's not *really* thinking like you or me, but it's really good at finding patterns and doing what it's been trained to do.\n\n**Important:** AI is made by people. So, it's important to use it wisely and make sure it's fair to everyone!  Just like you need to train your puppy kindly, AI needs to be made and used with kindness too.\n\nDoes that make sense?\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start a chat"
      ],
      "metadata": {
        "id": "hbQZTsixG3hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model = \"gemini-2.0-flash\", history = [])\n",
        "response = chat.send_message('Hello, my name is Ahmad')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvlI-MyAZNj",
        "outputId": "ed6994fe-b0a8-4ee2-d8cd-49b7fd4d5f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Ahmad, it's nice to meet you! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"can you tell me something interesting about dinosaurs\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZR6L3fHTEP",
        "outputId": "550dd0aa-427b-4214-eddb-ef3a513ae7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's an interesting fact about dinosaurs:\n",
            "\n",
            "**The Spinosaurus was likely the first known swimming dinosaur.**\n",
            "\n",
            "While many dinosaurs were believed to live near water sources, the Spinosaurus had features that suggested it was actively swimming and hunting in rivers. These features include:\n",
            "\n",
            "*   **Dense bones:** Unlike most dinosaurs which had hollow bones to reduce weight, Spinosaurus had dense bones similar to those of penguins, suggesting they were used as ballast for swimming.\n",
            "*   **Paddle-like tail:** Its tail was much taller and broader than other dinosaurs, shaped like a paddle to propel it through the water.\n",
            "*   **Nostrils positioned high on its skull:** This would allow it to breathe while partially submerged.\n",
            "*   **Isotope analysis of teeth:** Showed that it ate both aquatic and terrestrial prey.\n",
            "\n",
            "So, unlike the T-Rex which was a land-based predator, Spinosaurus seems to have adapted to a semi-aquatic lifestyle!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"do you still remember my name\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1O0TSqHef2",
        "outputId": "0dfe22c6-6436-408a-b91b-89471bc5a2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Ahmad! I remember your name.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chose a model from the Gemini model family\n",
        "\n",
        "Retrieves a list of available AI models, searches for a specific model named \"gemini-2.0-flash,\" and if found, displays its details in a formatted JSON-like structure. The search stops once the target model is located."
      ],
      "metadata": {
        "id": "uUBrfruvh7mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkU0vWoSglap",
        "outputId": "627df383-a6e7-4269-ab2d-ff91f6514994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='models/chat-bison-001' display_name='PaLM 2 Chat (Legacy)' description='A legacy text-only model optimized for chat conversations' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=4096 output_token_limit=1024 supported_actions=['generateMessage', 'countMessageTokens']\n",
            "name='models/text-bison-001' display_name='PaLM 2 (Legacy)' description='A legacy model that understands text and generates text as an output' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8196 output_token_limit=1024 supported_actions=['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens']\n",
            "name='models/gemini-1.0-pro-vision-latest' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-pro-vision' display_name='Gemini 1.0 Pro Vision' description='The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=12288 output_token_limit=4096 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-latest' display_name='Gemini 1.5 Pro Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-pro-001' display_name='Gemini 1.5 Pro 001' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro-002' display_name='Gemini 1.5 Pro 002' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-pro' display_name='Gemini 1.5 Pro' description='Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-latest' display_name='Gemini 1.5 Flash Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-001' display_name='Gemini 1.5 Flash 001' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-001-tuning' display_name='Gemini 1.5 Flash 001 Tuning' description='Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=16384 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createTunedModel']\n",
            "name='models/gemini-1.5-flash' display_name='Gemini 1.5 Flash' description='Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-002' display_name='Gemini 1.5 Flash 002' description='Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-1.5-flash-8b' display_name='Gemini 1.5 Flash-8B' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-001' display_name='Gemini 1.5 Flash-8B 001' description='Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-latest' display_name='Gemini 1.5 Flash-8B Latest' description='Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['createCachedContent', 'generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0827' display_name='Gemini 1.5 Flash 8B Experimental 0827' description='Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-1.5-flash-8b-exp-0924' display_name='Gemini 1.5 Flash 8B Experimental 0924' description='Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1000000 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.5-pro-exp-03-25' display_name='Gemini 2.5 Pro Experimental 03-25' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.5-pro-preview-03-25' display_name='Gemini 2.5 Pro Preview 03-25' description='Gemini 2.5 Pro Preview 03-25' version='2.5-preview-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent']\n",
            "name='models/gemini-2.0-flash-exp-image-generation' display_name='Gemini 2.0 Flash (Image Generation) Experimental' description='Gemini 2.0 Flash (Image Generation) Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.0 Flash Thinking Experimental 01-21' description='Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking' version='2.0-exp-01-21' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.0 Flash Thinking Experimental' description='Gemini 2.0 Flash Thinking Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/learnlm-1.5-pro-experimental' display_name='LearnLM 1.5 Pro Experimental' description='Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32767 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens']\n",
            "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent']\n",
            "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens']\n",
            "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer']\n",
            "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0 002 model' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=480 output_token_limit=8192 supported_actions=['predict']\n",
            "name='models/gemini-2.0-flash-live-001' display_name='Gemini 2.0 Flash 001' description='Gemini 2.0 Flash 001' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=131072 output_token_limit=8192 supported_actions=['bidiGenerateContent', 'countTokens']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "id": "CmeO6A6YIe0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5ec484-2b4a-4e4d-9932-2909df22a57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens', 'createCachedContent'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters Exploration"
      ],
      "metadata": {
        "id": "Fi_3neIHkzaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Length"
      ],
      "metadata": {
        "id": "MGm-Pg5OmE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the max number of output token to 200\n",
        "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
        "\n",
        "# create a response\n",
        "response_short = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = short_config,\n",
        "    contents='write a short essay on the importance of education in modern day society.'\n",
        ")\n",
        "\n",
        "print(response_short.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpL4Sb2oiu7r",
        "outputId": "52b0574f-5083-43e6-c531-3fe047f177b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Bedrock of Progress: The Enduring Importance of Education in Modern Society\n",
            "\n",
            "In the complex and rapidly evolving landscape of modern society, education stands as a cornerstone of progress, personal development, and societal advancement. More than simply acquiring knowledge, education empowers individuals, strengthens communities, and fuels innovation, making it an indispensable tool for navigating the challenges and opportunities of the 21st century.\n",
            "\n",
            "Firstly, education is paramount for individual empowerment. It equips individuals with the critical thinking skills necessary to analyze information, solve problems, and make informed decisions. Literacy and numeracy, the fundamental building blocks of education, unlock access to information and participation in civic life. Beyond these basics, specialized education and training open doors to diverse career paths, providing individuals with the means to achieve economic independence and pursue their passions. Education fosters self-confidence, resilience, and a lifelong thirst for learning, enabling individuals to adapt to changing circumstances and contribute meaningfully to society.\n",
            "\n",
            "Furthermore, education plays a crucial role in fostering social cohesion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "How much randomness is included when selecting the next word (token) in language models depends on the \"temperature\" parameter.  A higher temperature causes the model to take into account a greater number of potential words, producing more inventive and diverse results.  On the other hand, a lower temperature forces the model to adhere to the most likely terms, producing language that is more concentrated and predictable.  The model turns completely deterministic when the temperature is set to 0, always choosing the word that is the most likely.  Temperature does not, however, ensure actual randomness; rather, it is a means of directing the model toward more or less random results."
      ],
      "metadata": {
        "id": "ErIkZR5snjVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVK4xyumIuX",
        "outputId": "c6e344e2-cd01-4631-eda7-405b80fb192f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Cerulean\n",
            " _________________________\n",
            "Mauve\n",
            " _________________________\n",
            "Turquoise\n",
            " _________________________\n",
            "Magenta.\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous output shows the randomness but we would experiment with temperature value of 0. we can see from the output that it is more direct and precise and there is no randomness."
      ],
      "metadata": {
        "id": "_zRPP_mrs-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPCPP9Mox8R",
        "outputId": "df129ceb-9510-4d00-8b33-ea51537ae97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top P"
      ],
      "metadata": {
        "id": "8Y-BK9xPuN9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 1.0, # default config\n",
        "    top_p = 0.95       # default config\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a senior network engineer. Explain ospf to a 5 year old\"\n",
        "response_topp = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents=story_prompt\n",
        ")\n",
        "\n",
        "print(response_topp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9LE2k_7t098",
        "outputId": "612e20a7-e6c2-4530-c833-2b4c5aa920b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine we're playing a game with toy cars, and each car needs to deliver a package to different houses in our neighborhood.\n",
            "\n",
            "You and your friends each have a toy car and a map of the neighborhood.  But everyone's map is a little different!\n",
            "\n",
            "OSPF is like a special club we join so that EVERYONE has the *best* map of the neighborhood.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1. **Cars are like routers:** Each toy car is like a router. A router is like a tiny computer that helps messages (like your packages) travel from one place to another.\n",
            "\n",
            "2. **Houses are like networks:** Each house is like a network – a group of computers or devices that are connected together.\n",
            "\n",
            "3. **Roads are like connections:** The roads connecting the houses are like the connections between our routers.\n",
            "\n",
            "4. **Sharing information:** OSPF helps all the toy cars (routers) talk to each other.  They say things like, \"Hey, I know a shortcut to get to that house!  It's this way...\" and \"This road is closed because a giant teddy bear is blocking it!\"\n",
            "\n",
            "5. **Building the best map:**  Every toy car listens to the others and updates their map.  That way, everyone knows the best and fastest way to get to any house. They all have the same, most accurate map.\n",
            "\n",
            "6. **Finding the fastest path:** With the best map, each toy car can figure out the quickest and easiest route to deliver its package.\n",
            "\n",
            "**So, basically, OSPF is like a way for all the toy cars (routers) to share information about the neighborhood (network) so they can all deliver their packages (messages) as quickly and efficiently as possible!**\n",
            "\n",
            "Does that make sense?  Would you like to play the toy car game now?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting"
      ],
      "metadata": {
        "id": "A8rqMwXfHN3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot Prompt"
      ],
      "metadata": {
        "id": "aZLlvtk0HRao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the prompt parameters\n",
        "model_config = types.GenerateContentConfig(\n",
        "  temperature = 0.1,\n",
        "  top_p = 1,\n",
        "  max_output_tokens = 5\n",
        ")\n",
        "\n",
        "# set the prompt\n",
        "zero_short_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response_zero_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "\n",
        "print(response_zero_shot.text) # print the output as text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BjVXOH_m7Y",
        "outputId": "80fd8024-f577-4002-a7f7-cf77c24da045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enum Mode\n",
        "The Sentiment enum acts as a schema or a constraint. It tells the language model that the expected output should be one of these three predefined sentiment categories.\n",
        "\n",
        "The `text/x.enum` MIME type indicates that the response should be one of the values defined in the response_schema."
      ],
      "metadata": {
        "id": "B1qz1KnEKK5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new enumeration class tha inherit from enum.Enum\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'positive'\n",
        "  NEUTRAL = 'neutral'\n",
        "  NEGATIVE = 'negative'\n",
        "\n",
        "response_zero_short_enum = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "\n",
        "        # tell the model the ezpected output format and that the rsponse should\n",
        "        # be one of thos defined in the schema\n",
        "        response_mime_type = 'text/x.enum',\n",
        "\n",
        "        # set the Sentiment as schema\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "print(response_zero_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iJTirTJpqN",
        "outputId": "74820e71-2615-4cc6-c1e2-5f5eab97a037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Short Prompt"
      ],
      "metadata": {
        "id": "3mVeGOaOc4l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = 'Give me a large with cheese and pineapple'\n",
        "\n",
        "response_one_few_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response_one_few_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UOTJ-HQojw",
        "outputId": "f6b4e44c-7177-45d7-f63d-09a393f8a919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JSON Mode\n",
        "\n",
        "`class PizzaOrder(typing.TypedDict):`: This defines a new class called PizzaOrder that inherits from typing.TypedDict. TypedDict is used to create dictionary types where the keys and their corresponding value types are known\n",
        "\n",
        "Purpose of the `TypedDict` is to act as a schema. It defines the structure and data types of the JSON output we expect from the language model. This allows us to work with the model's response in a structured and predictable way.\n"
      ],
      "metadata": {
        "id": "E0OeKzzZ4-S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PizzaOrder(typing.TypedDict):\n",
        "  size:str\n",
        "  ingredients: list[str]\n",
        "  type:str\n",
        "\n",
        "response_json = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 0.1,\n",
        "        response_mime_type = 'application/json',\n",
        "        response_schema = PizzaOrder\n",
        "    ),\n",
        "\n",
        "    contents = \"Can I have a large dessert pizza with apple and chocolate\"\n",
        ")\n",
        "\n",
        "print(response_json.text)"
      ],
      "metadata": {
        "id": "pJ-0mRg3d4L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cd0b95-3062-4354-860f-695d17fb151c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought\n",
        "\n",
        "Chain of Thought prompting improves the language model's ability to perform complex reasoning tasks. By explicitly requesting the model to show its reasoning process, it's more likely to arrive at the correct answer, as it avoids impulsive or incorrect conclusions."
      ],
      "metadata": {
        "id": "UCwQgelU1eXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the prompt\n",
        "cot_prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? let's think step by step.\"\"\"\n",
        "\n",
        "response_cot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = cot_prompt)\n",
        "\n",
        "Markdown(response_cot.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "2QOblph7-QL1",
        "outputId": "84d50f15-ac23-41e4-8ce5-8be15ac7753d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3. **Determine the partner's current age:** Since the age difference remains constant, your partner is always 8 years older than you. Now that you are 20, your partner is 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**\n"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reason and Act: ReAct"
      ],
      "metadata": {
        "id": "6wxssztRDiJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "lwSgOshf2BqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the question\n",
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences = ['\\nObservation'],\n",
        "    system_instruction = model_instructions + example1 + example2\n",
        "    )\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_response = client.chats.create(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = react_config\n",
        ")\n",
        "\n",
        "resp = react_response.send_message(question)\n",
        "\n",
        "print(resp.text)"
      ],
      "metadata": {
        "id": "nd-5HwoKEhWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c61368-b41d-4c62-ea4b-b911b1b53230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the transformers NLP paper, find the authors, and identify the youngest one.\n",
            "\n",
            "Action 1\n",
            "<search>transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thinking Mode\n"
      ],
      "metadata": {
        "id": "9pOpInq9c5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a response\n",
        "response_thinking = client.models.generate_content_stream(\n",
        "    model = 'gemini-2.0-flash-thinking-exp',\n",
        "    contents = 'Who was the youngest athor listed on the transofrmer NLP paper'\n",
        ")\n",
        "\n",
        "buffer = io.StringIO()\n",
        "for chunk in response_thinking:\n",
        "  buffer.write(chunk.text)\n",
        "\n",
        "  # display rsponse as it is streamed\n",
        "  print(chunk.text, end = '')\n",
        "\n",
        "# render the finished response as formatted markdown\n",
        "clear_output()\n",
        "Markdown(buffer.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "VVV2vqUbKvGW",
        "outputId": "3bae9cbc-d870-4177-a3b3-8127fc49b910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The youngest author listed on the \"Attention is All You Need\" paper (which introduced the Transformer model) is **Aidan N. Gomez**.\n\nWhile precise birthdates aren't always publicly available for researchers, based on publicly available information such as:\n\n* **His academic trajectory:** He was a PhD student at the University of Oxford at the time of the paper's publication (2017). PhD students are generally younger than professors or established researchers.\n* **His subsequent career stage:**  He co-founded Cohere, an AI company, after his PhD. This is typical for researchers who are earlier in their careers and looking to build upon their research.\n* **General online presence and professional profiles:**  His profile generally suggests a more recent entry into the field compared to some of the other authors who are more established figures.\n\nThe other authors on the paper are more senior researchers with longer academic or industry careers, making Aidan N. Gomez the most likely youngest author at the time of publication.\n\nTherefore, **Aidan N. Gomez** is widely considered to be the youngest author on the Transformer paper."
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99SPUqzieldE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}