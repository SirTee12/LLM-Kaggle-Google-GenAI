{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBwzY/EAkte+QdEDkOX5R6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirTee12/LLM-Kaggle-Google-GenAI/blob/main/Prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the SDK"
      ],
      "metadata": {
        "id": "QJ85fhKIppqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e7_t8fZxbZMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297455ba-6db2-42f9-bf74-f23130ef75be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy jupyterlab # uninstall jupyterlab in the case it conflict google colab base image\n",
        "!pip install -U -q \"google-genai==1.7.0\" # install google genai library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the SDK and some helpers for rendering the output. The `types` in `import types` may include custom data structures, classes or type hints psecifically to work with google AI. These custom types might represent things like model parameters, input/output formats for AI models or config settings."
      ],
      "metadata": {
        "id": "jkGlMbU9ueF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import enum\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "vn5P5o2Zp4Jd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Call Retry Implementation\n",
        "Set up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota\n",
        "\n",
        "\n",
        "1.  **Imports `retry`:** It imports the `retry` module from `google.api_core`, which provides tools for automatic retries of API calls.\n",
        "\n",
        "2.  **Defines `is_retriable`:**\n",
        "    * It creates a lambda function called `is_retriable`.\n",
        "    * This function checks if an exception (`e`) is an instance of `genai.errors.APIErrpr` and if its error code (`e.code`) is either 429 (Too Many Requests) or 503 (Service Unavailable).\n",
        "    * It returns `True` if both conditions are met (meaning the API call is considered retriable), and `False` otherwise.\n",
        "\n",
        "3.  **Applies Retry Logic:**\n",
        "    * It modifies the `genai.models.Models.generate_content` function by wrapping it with the `retry.retry` function.\n",
        "    * The `retry.retry` function is configured with the `is_retriable` function as the `predicate`. This tells `retry.retry` to only retry the API call if the `is_retriable` function returns `True` for the encountered exception.\n",
        "    * Essentially, this replaces the original `generate_content` function with a new version that automatically retries on 429 and 503 errors, making the code more robust to temporary API issues."
      ],
      "metadata": {
        "id": "JCueWvEs3iyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in (429, 503))\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(predicate = is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "tyJcQ1EW3tec"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"secret-label-prompt\")"
      ],
      "metadata": {
        "id": "8jzS_Om16DxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ceb1f515-75dc-4fd8-b0ee-e1b5e84c93c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kaggle_secrets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-215ab66f97c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msecret_value_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"secret-label-prompt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the API keys and create a client\n",
        "\n",
        "Retrieves the API key securely and create a client object that will be used to communicate with the generative AI service, using the retrieved API key for authentication."
      ],
      "metadata": {
        "id": "qmMX2ZA28UP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY') # retrive the APi key\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)   # create a client to interact with the genai application"
      ],
      "metadata": {
        "id": "VpfMD3Jo7_IX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the First Prompt"
      ],
      "metadata": {
        "id": "vaCFpBre_3sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a response\n",
        "response = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    contents = \"Explain AI to me like I'm a kid\"\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "-zXMiooZ_pMB",
        "outputId": "49c6e5cf-9d5b-4e61-fb18-557f632a2972"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a really, really smart puppy named Sparky. Sparky can learn new tricks, but instead of treats, he learns from lots and lots of examples! That's kind of like AI.\n",
            "\n",
            "AI stands for Artificial Intelligence. It means making computers smart like people, but in a different way.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "*   **Show Sparky lots of pictures of cats and dogs:** If you show Sparky thousands of pictures of cats and dogs and tell him which is which, he starts to learn the difference! He might notice that cats usually have pointier ears.\n",
            "\n",
            "*   **Ask Sparky to guess:** Now you show Sparky a new picture of an animal he's never seen before. Sparky might say, \"Hmm, it has pointy ears and whiskers, so I think it's a cat!\" He's making a guess based on what he learned.\n",
            "\n",
            "*   **Tell Sparky if he's right or wrong:** If Sparky guessed right, you say \"Good boy!\" If he guessed wrong, you tell him the correct answer. Each time he learns, he gets a little bit smarter and better at guessing.\n",
            "\n",
            "That's what AI does! It looks at tons of information, learns patterns, and then uses those patterns to make guesses or solve problems.\n",
            "\n",
            "So, AI is like a really smart puppy that learns from lots of examples to do things like:\n",
            "\n",
            "*   **Recognize your voice on your phone:** When you say \"Hey Siri,\" your phone is using AI to understand what you're saying.\n",
            "*   **Suggest videos for you to watch on YouTube:** YouTube's AI knows what kind of videos you like based on what you've watched before.\n",
            "*   **Help doctors diagnose diseases:** AI can look at medical images and help doctors find problems that might be hard to see.\n",
            "*   **Drive cars:** Some cars are being developed with AI to drive themselves!\n",
            "\n",
            "AI isn't magic, but it's pretty cool! It's all about teaching computers to learn and make decisions like we do, but using lots and lots of information.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a really, really smart puppy named Sparky. Sparky can learn new tricks, but instead of treats, he learns from lots and lots of examples! That's kind of like AI.\n\nAI stands for Artificial Intelligence. It means making computers smart like people, but in a different way.\n\nHere's how it works:\n\n*   **Show Sparky lots of pictures of cats and dogs:** If you show Sparky thousands of pictures of cats and dogs and tell him which is which, he starts to learn the difference! He might notice that cats usually have pointier ears.\n\n*   **Ask Sparky to guess:** Now you show Sparky a new picture of an animal he's never seen before. Sparky might say, \"Hmm, it has pointy ears and whiskers, so I think it's a cat!\" He's making a guess based on what he learned.\n\n*   **Tell Sparky if he's right or wrong:** If Sparky guessed right, you say \"Good boy!\" If he guessed wrong, you tell him the correct answer. Each time he learns, he gets a little bit smarter and better at guessing.\n\nThat's what AI does! It looks at tons of information, learns patterns, and then uses those patterns to make guesses or solve problems.\n\nSo, AI is like a really smart puppy that learns from lots of examples to do things like:\n\n*   **Recognize your voice on your phone:** When you say \"Hey Siri,\" your phone is using AI to understand what you're saying.\n*   **Suggest videos for you to watch on YouTube:** YouTube's AI knows what kind of videos you like based on what you've watched before.\n*   **Help doctors diagnose diseases:** AI can look at medical images and help doctors find problems that might be hard to see.\n*   **Drive cars:** Some cars are being developed with AI to drive themselves!\n\nAI isn't magic, but it's pretty cool! It's all about teaching computers to learn and make decisions like we do, but using lots and lots of information.\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start a chat"
      ],
      "metadata": {
        "id": "hbQZTsixG3hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model = \"gemini-2.0-flash\", history = [])\n",
        "response = chat.send_message('Hello, my name is Ahmad')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnvlI-MyAZNj",
        "outputId": "b2c9d939-1255-478c-c396-387110eac274"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Ahmad, it's nice to meet you! How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"can you tell me something interesting about dinosaurs\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZR6L3fHTEP",
        "outputId": "187ead04-eace-4838-b373-a317fbdd1e15"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here's a fascinating fact about dinosaurs:\n",
            "\n",
            "**Many dinosaurs, including the mighty Tyrannosaurus Rex, likely had feathers!**\n",
            "\n",
            "While the image of dinosaurs is often scaly and reptilian, more and more evidence shows that feathers were much more widespread than previously thought. Scientists have found fossil evidence of feathered dinosaurs, and some even had wings! These feathers likely served different purposes:\n",
            "\n",
            "*   **Insulation:** To keep them warm.\n",
            "*   **Display:** To attract mates or intimidate rivals.\n",
            "*   **Camouflage:** To blend in with their environment.\n",
            "\n",
            "So, the next time you picture a T-Rex, imagine it with a fluffy (or even brightly colored) coat of feathers!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"do you still remember my name\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1O0TSqHef2",
        "outputId": "518d7318-1119-454a-8a7d-e90215778657"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Ahmad! I remember your name is Ahmad. Is there anything else I can help you with, Ahmad?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chose a model from the Gemini model family\n",
        "\n",
        "Retrieves a list of available AI models, searches for a specific model named \"gemini-2.0-flash,\" and if found, displays its details in a formatted JSON-like structure. The search stops once the target model is located."
      ],
      "metadata": {
        "id": "uUBrfruvh7mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "for model in client.models.list():\n",
        "  if model.name == \"models/gemini-2.0-flash\":\n",
        "    pprint(model.to_json_dict())\n",
        "    break"
      ],
      "metadata": {
        "id": "CmeO6A6YIe0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f043a43e-4d08-4b32-f402-0a630152bf9e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': 'Gemini 2.0 Flash',\n",
            " 'display_name': 'Gemini 2.0 Flash',\n",
            " 'input_token_limit': 1048576,\n",
            " 'name': 'models/gemini-2.0-flash',\n",
            " 'output_token_limit': 8192,\n",
            " 'supported_actions': ['generateContent', 'countTokens'],\n",
            " 'tuned_model_info': {},\n",
            " 'version': '2.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters Exploration"
      ],
      "metadata": {
        "id": "Fi_3neIHkzaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Length"
      ],
      "metadata": {
        "id": "MGm-Pg5OmE3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the max number of output token to 200\n",
        "short_config = types.GenerateContentConfig(max_output_tokens = 200)\n",
        "\n",
        "# create a response\n",
        "response_short = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = short_config,\n",
        "    contents='write a short essay on the importance of education in modern day society.'\n",
        ")\n",
        "\n",
        "print(response_short.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpL4Sb2oiu7r",
        "outputId": "ff22a66f-e6c2-4151-bd93-ce382232954f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Cornerstone of Progress: Education in Modern Society\n",
            "\n",
            "In the complex and rapidly evolving landscape of modern society, education stands not merely as an advantage, but as an indispensable cornerstone. Its importance extends far beyond the acquisition of knowledge and skills, serving as the engine of individual empowerment, societal progress, and global competitiveness. Investing in education is, therefore, an investment in a brighter and more prosperous future.\n",
            "\n",
            "For the individual, education provides the tools necessary to navigate the complexities of modern life. It equips individuals with critical thinking skills, enabling them to analyze information, solve problems, and make informed decisions. Literacy and numeracy, the foundational building blocks of education, are essential for participation in the workforce, accessing essential services, and understanding the world around them. Beyond these basics, education cultivates intellectual curiosity, fosters creativity, and promotes personal growth. It empowers individuals to pursue their passions, develop their potential, and lead fulfilling lives.\n",
            "\n",
            "Furthermore, education plays a pivotal role in driving societal progress. A well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "How much randomness is included when selecting the next word (token) in language models depends on the \"temperature\" parameter.  A higher temperature causes the model to take into account a greater number of potential words, producing more inventive and diverse results.  On the other hand, a lower temperature forces the model to adhere to the most likely terms, producing language that is more concentrated and predictable.  The model turns completely deterministic when the temperature is set to 0, always choosing the word that is the most likely.  Temperature does not, however, ensure actual randomness; rather, it is a means of directing the model toward more or less random results."
      ],
      "metadata": {
        "id": "ErIkZR5snjVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 2.0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQVK4xyumIuX",
        "outputId": "5865536f-305b-4dd2-87d2-80ae4d8fa3a2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turquoise\n",
            " _________________________\n",
            "Orange.\n",
            " _________________________\n",
            "Magenta\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Orange\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous output shows the randomness but we would experiment with temperature value of 0. we can see from the output that it is more direct and precise and there is no randomness."
      ],
      "metadata": {
        "id": "_zRPP_mrs-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a variable to set the temperature\n",
        "high_temp_config = types.GenerateContentConfig(temperature = 0)\n",
        "\n",
        "for _ in range(5):\n",
        "  response_temp = client.models.generate_content(\n",
        "      model = 'gemini-2.0-flash',\n",
        "      config = high_temp_config,\n",
        "      contents = 'Pick a random colour... (respond in a single word)'\n",
        "  )\n",
        "\n",
        "  if response_temp.text:\n",
        "    print(response_temp.text, '_' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPCPP9Mox8R",
        "outputId": "a868d616-b646-427b-a7ee-21fb6192c3dc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n",
            "Azure\n",
            " _________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top P"
      ],
      "metadata": {
        "id": "8Y-BK9xPuN9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature = 1.0, # default config\n",
        "    top_p = 0.95       # default config\n",
        ")\n",
        "\n",
        "story_prompt = \"You are a senior network engineer. Explain ospf to a 5 year old\"\n",
        "response_topp = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents=story_prompt\n",
        ")\n",
        "\n",
        "print(response_topp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9LE2k_7t098",
        "outputId": "cee0a98f-d390-44cf-c5ff-71076eca3fef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine the internet is like a big neighborhood, and each house is a computer or device. And you want to send a letter (data) from your house to your friend's house across the neighborhood.\n",
            "\n",
            "Now, OSPF is like a special group of mailmen. These mailmen are really smart! Instead of just guessing the best way to deliver the letter, they talk to each other.\n",
            "\n",
            "Here's how it works:\n",
            "\n",
            "1. **\"Hello, I'm here!\"**: Each mailman (router) tells all the mailmen nearby, \"Hi! I'm here! These are the houses I can deliver to directly.\" They tell everyone their address, and how long it takes to walk to those houses. It's like saying, \"I can deliver to the house next door in one minute!\"\n",
            "\n",
            "2. **Sharing the Map**: Each mailman writes down all the information they get from the other mailmen. So now they have a map of the whole neighborhood!  This map tells them which mailmen can reach which houses and how long it takes.\n",
            "\n",
            "3. **Finding the Best Path**: When you give a letter to a mailman, they look at their map and figure out the fastest and easiest way to get the letter to your friend's house.  They choose the path that takes the least time, even if it means handing the letter off to another mailman along the way.  It's like saying, \"Okay, to get to your friend's house, I should give this letter to the mailman down the street, because THEY know how to get there faster!\"\n",
            "\n",
            "4. **Adjusting the Map**: If a road gets blocked (a link goes down), or a new shortcut is built (a new link is added), the mailmen tell each other right away.  Then they all update their maps so they can still find the best way to deliver the letters.\n",
            "\n",
            "**So, OSPF helps computers talk to each other by:**\n",
            "\n",
            "*   **Sharing information** about the network (the neighborhood).\n",
            "*   **Building a map** of the network.\n",
            "*   **Finding the best path** to send information (the letters) quickly and reliably.\n",
            "*   **Adjusting to changes** in the network so messages always get through.\n",
            "\n",
            "It's like having a team of super-efficient mailmen who always know the best way to deliver your letter, no matter what!  And that's why OSPF is important for making sure the internet works smoothly.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting"
      ],
      "metadata": {
        "id": "A8rqMwXfHN3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot Prompt"
      ],
      "metadata": {
        "id": "aZLlvtk0HRao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the prompt parameters\n",
        "model_config = types.GenerateContentConfig(\n",
        "  temperature = 0.1,\n",
        "  top_p = 1,\n",
        "  max_output_tokens = 5\n",
        ")\n",
        "\n",
        "# set the prompt\n",
        "zero_short_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response_zero_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = model_config,\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "\n",
        "print(response_zero_shot.text) # print the output as text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BjVXOH_m7Y",
        "outputId": "c408f6d9-863f-4409-dc81-0bf496fb0221"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enum Mode\n",
        "The Sentiment enum acts as a schema or a constraint. It tells the language model that the expected output should be one of these three predefined sentiment categories.\n",
        "\n",
        "The `text/x.enum` MIME type indicates that the response should be one of the values defined in the response_schema."
      ],
      "metadata": {
        "id": "B1qz1KnEKK5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new enumeration class tha inherit from enum.Enum\n",
        "class Sentiment(enum.Enum):\n",
        "  POSITIVE = 'positive'\n",
        "  NEUTRAL = 'neutral'\n",
        "  NEGATIVE = 'negative'\n",
        "\n",
        "response_zero_short_enum = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "\n",
        "        # tell the model the ezpected output format and that the rsponse should\n",
        "        # be one of thos defined in the schema\n",
        "        response_mime_type = 'text/x.enum',\n",
        "\n",
        "        # set the Sentiment as schema\n",
        "        response_schema = Sentiment\n",
        "    ),\n",
        "\n",
        "    contents = zero_short_prompt\n",
        ")\n",
        "print(response_zero_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iJTirTJpqN",
        "outputId": "d21697f5-abdd-4642-d46d-5990f1ad8fc5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Short Prompt"
      ],
      "metadata": {
        "id": "3mVeGOaOc4l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = 'Give me a large with cheese and pineapple'\n",
        "\n",
        "response_one_few_shot = client.models.generate_content(\n",
        "    model = 'gemini-2.0-flash',\n",
        "    config = types.GenerateContentConfig(\n",
        "        temperature = 1,\n",
        "        top_p = 1,\n",
        "        max_output_tokens = 250\n",
        "    ),\n",
        "    contents = [few_shot_prompt, customer_order]\n",
        ")\n",
        "\n",
        "print(response_one_few_shot.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4UOTJ-HQojw",
        "outputId": "7e488dd6-26b0-446b-c144-363a14421751"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJ-0mRg3d4L3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}